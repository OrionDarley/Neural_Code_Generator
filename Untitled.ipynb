{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import distance\n",
    "import collections\n",
    "import configargparse\n",
    "from tqdm import tqdm\n",
    "from  more_itertools import unique_everseen\n",
    "\n",
    "import opts\n",
    "\n",
    "from flask import Flask, render_template, url_for, redirect, jsonify, request  # noqa\n",
    "from flask_cors import CORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Flask setup\n",
    "# app = Flask(__name__)\n",
    "# CORS(app)\n",
    "\n",
    "# app.config['SECRET_KEY'] = 'fe093b0354f9ba0b1237a5e36f58caf55cc9f5682c2627b7463c30f0bbd97672'  # noqa\n",
    "\n",
    "\n",
    "# Regex\n",
    "newline_ptr = re.compile(r'(?:\"[^\"]*\"|.)+')  # \\n outside of quotes (https://stackoverflow.com/questions/24018577/parsing-a-string-in-python-how-to-split-newlines-while-ignoring-newline-inside)\n",
    "# comment_ptr = re.compile(r'\"\"\"(.*?)\"\"\"|\\'\\'\\'(.*?)\\'\\'\\'|#[^\"\\']*?(?=\\n)|#.*?(\".*?\"|\\'.*?\\').*?(?=\\n)', re.DOTALL|re.MULTILINE)\n",
    "comment_ptr = re.compile(r'\\'\\'\\'(.*?)\\'\\'\\'|#[^\"\\']*?(?=\\n)|#.*?(\".*?\"|\\'.*?\\').*?(?=\\n)', re.DOTALL|re.MULTILINE)\n",
    "literal_ptr = re.compile(r'\".*?\"|\\'.*?\\'|[-+]?\\d*\\.\\d+|\\d+')\n",
    "camelcase_ptr = re.compile(r\"(?<=[a-z])([A-Z]+[a-z]*)\")\n",
    "number_ptr = re.compile(r'(?<=[^a-zA-Z])([-+]?\\d*\\.\\d+|\\d+)')\n",
    "number_with_alpha_ptr = re.compile(r'(?<=[a-zA-Z])([-+]?\\d*\\.\\d+|\\d+)')  # split numbers from alpha\n",
    "string_ptr = re.compile(r'\".*?\"|\\'.*?\\'')\n",
    "code_ptr = re.compile(r\"([^a-zA-Z0-9])\")\n",
    "whitespace_ptr = re.compile(r\"(\\s+)\")\n",
    "\n",
    "\n",
    "# Read default options\n",
    "# parser = configargparse.ArgumentParser(description=\"index.py\")\n",
    "# opts.system_opts(parser)\n",
    "# opt = parser.parse_args()\n",
    "\n",
    "results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Read data\n",
    "###############################################################################\n",
    "\n",
    "def preprocess_source(source):\n",
    "    \"\"\"\n",
    "    Remove # in strings for comment_ptr.\n",
    "\n",
    "    TODO. Later on, fix regex instead.\n",
    "    \"\"\"\n",
    "    for string in re.findall(r'\".*?#.*?\"|\\'.*?#.*?\\'', source):\n",
    "        if '#' in string:\n",
    "            modified = string.replace('#', '')\n",
    "            source = source.replace(string, modified)\n",
    "    return source\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"[!] Data does not exist\")\n",
    "    elif os.path.isfile(path):\n",
    "        return read_file(path)\n",
    "    else:\n",
    "        return read_dir(path)\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    num_file = 1\n",
    "    sources = []\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            source.append(preprocess_source(f.read()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return sources, num_file\n",
    "\n",
    "\n",
    "def read_dir(path):\n",
    "    num_file = 0\n",
    "    sources = []\n",
    "    for filename in glob.iglob(os.path.join(path, \"**/*.py\"), recursive=True):\n",
    "#         print(filename)\n",
    "        num_file += 1\n",
    "        with open(filename, \"r\") as f:\n",
    "            sources.append(preprocess_source(f.read()))\n",
    "    return sources, num_file\n",
    "\n",
    "\n",
    "def read_from_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_as_pickle(path, data):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../scrape_github/electron\"\n",
    "\n",
    "\n",
    "# sources, num_files = read_data(opt.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\"debug\": True, \"save\": True, \"overwrite\": True, \"verbose\": True, \"unit\": \"line\", \"min_len\": 10, \"distance_metric_x\": \"nc\",\n",
    "          \"distance_threshold_x\": 0.3, \"distance_metric_y\": \"c\", \"distance_threshold_y\": 2, \"n_leftmost_tokens\": 1, \"max_num_candidates\": 1,\n",
    "      \"check_exact_match_suffix\": True}\n",
    "from collections import namedtuple\n",
    "MyStruct = namedtuple('MyStruct', 'path debug save overwrite verbose unit min_len distance_metric_x distance_threshold_x distance_metric_y distance_threshold_y n_leftmost_tokens max_num_candidates check_exact_match_suffix')\n",
    "\n",
    "opt = MyStruct(path = path, debug = True, save = True, overwrite = True, verbose = True, unit = \"line\", min_len = 10, distance_metric_x = \"nc\",\n",
    "          distance_threshold_x = 0.3, distance_metric_y = \"c\", distance_threshold_y = 2, n_leftmost_tokens = 1, max_num_candidates = 1, \n",
    "               check_exact_match_suffix = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments(source):\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", re.sub(comment_ptr, \"\", source))\n",
    "\n",
    "def get_lines_from_source(source,\n",
    "                          remove_comments_from_source,\n",
    "                          remove_empty_lines_from_source):\n",
    "    \"\"\"\n",
    "    Remove comments and empty lines from source.\n",
    "    Return a list of lines\n",
    "    \"\"\"\n",
    "    if remove_comments_from_source:\n",
    "        source = remove_comments(source)\n",
    "    \n",
    "    lines = split_newlines(source)\n",
    "\n",
    "    if remove_empty_lines_from_source:\n",
    "        lines = [line for line in lines if len(line.strip()) > 0]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(sources):\n",
    "    \"\"\"\n",
    "    From a project, extract all (x, y) pairs\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    num_lines = 0\n",
    "    for source in sources:\n",
    "\n",
    "        num_lines += len(get_lines_from_source(source,\n",
    "                                               remove_comments_from_source=True,\n",
    "                                               remove_empty_lines_from_source=True))\n",
    "        return source\n",
    "        examples = extract_examples_line_by_line(source)\n",
    "#         print(examples)\n",
    "        dataset.extend(examples)\n",
    "\n",
    "    print(f\"\\n[construct_dataset] Number of lines in the project: {num_lines}\")\n",
    "    print(f\"[construct_dataset] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(opt):\n",
    "    \"\"\"\n",
    "    Check if processed datasets exist.\n",
    "\n",
    "    If so, read. Otherwise, generate and save to the path.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[generate_datasets] Processing {opt.path}\")\n",
    "    options = to_string_opt(opt)\n",
    "    path_dataset = os.path.join(opt.path, f\"dataset_{options}.p\")\n",
    "    path_dataset_edit = os.path.join(opt.path, f\"dataset_edit_{options}.p\")\n",
    "\n",
    "    if os.path.exists(path_dataset) and os.path.exists(path_dataset_edit) and not opt.overwrite:\n",
    "        print(\"[generate_datasets] Read from pickled files\")\n",
    "        dataset = read_from_pickle(path_dataset)\n",
    "        dataset_edit = read_from_pickle(path_dataset_edit)\n",
    "    else:\n",
    "        sources, num_file = read_data(opt.path)\n",
    "\n",
    "        dataset = construct_dataset(sources)  # D_proj = {(x, y)}\n",
    "        return dataset\n",
    "        if len(dataset) > 10000:\n",
    "            print(f\"[generate_datasets] Skipping too large project (|dataset| = {len(dataset)})\")\n",
    "            dataset_edit = []\n",
    "        else:\n",
    "            dataset_edit = construct_dataset_edit(opt, dataset)  # D_edit = {(x, y, x', y')}\n",
    "\n",
    "            # Save datasets for future use\n",
    "            if opt.save:\n",
    "                save_as_pickle(path_dataset, dataset)\n",
    "                save_as_pickle(path_dataset_edit, dataset_edit)\n",
    "\n",
    "    print(f\"\\n[generate_datasets] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "    print(f\"[generate_datasets] Number of examples for edit: {len(dataset_edit)} ({len(set(dataset_edit))} unique examples)\\n\")\n",
    "\n",
    "    return dataset, dataset_edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[generate_datasets] Processing ../scrape_github/electron\n",
      "../scrape_github/electron/tools/js2c.py\n",
      "../scrape_github/electron/script/apply_all_patches.py\n",
      "../scrape_github/electron/script/add-debug-link.py\n",
      "../scrape_github/electron/script/strip-binaries.py\n",
      "../scrape_github/electron/script/check-trailing-whitespace.py\n",
      "../scrape_github/electron/script/check-relative-doc-links.py\n",
      "../scrape_github/electron/script/verify-chromedriver.py\n",
      "../scrape_github/electron/script/verify-mksnapshot.py\n",
      "../scrape_github/electron/script/native-tests.py\n",
      "../scrape_github/electron/script/zip-symbols.py\n",
      "../scrape_github/electron/script/run-gn-format.py\n",
      "../scrape_github/electron/script/run-clang-format.py\n",
      "../scrape_github/electron/script/dbus_mock.py\n",
      "../scrape_github/electron/script/generate-zip-manifest.py\n",
      "../scrape_github/electron/script/verify-ffmpeg.py\n",
      "../scrape_github/electron/script/update-external-binaries.py\n",
      "../scrape_github/electron/script/copy-debug-symbols.py\n",
      "../scrape_github/electron/script/release/merge-electron-checksums.py\n",
      "../scrape_github/electron/script/release/uploaders/upload-index-json.py\n",
      "../scrape_github/electron/script/release/uploaders/upload-node-checksums.py\n",
      "../scrape_github/electron/script/release/uploaders/upload.py\n",
      "../scrape_github/electron/script/release/uploaders/upload-node-headers.py\n",
      "../scrape_github/electron/script/release/uploaders/upload-symbols.py\n",
      "../scrape_github/electron/script/lib/git.py\n",
      "../scrape_github/electron/script/lib/config.py\n",
      "../scrape_github/electron/script/lib/env_util.py\n",
      "../scrape_github/electron/script/lib/util.py\n",
      "../scrape_github/electron/script/lib/__init__.py\n",
      "../scrape_github/electron/script/lib/npm.py\n",
      "../scrape_github/electron/script/lib/gn.py\n",
      "../scrape_github/electron/script/lib/patches.py\n",
      "../scrape_github/electron/script/lib/npx.py\n",
      "../scrape_github/electron/script/lib/native_tests.py\n",
      "../scrape_github/electron/script/zip_manifests/check-zip-manifest.py\n",
      "../scrape_github/electron/build/generate-template.py\n",
      "../scrape_github/electron/build/run-in-dir.py\n",
      "../scrape_github/electron/build/profile_toolchain.py\n",
      "../scrape_github/electron/build/dump_syms.py\n",
      "../scrape_github/electron/build/run-node.py\n",
      "../scrape_github/electron/build/zip.py\n",
      "../scrape_github/electron/build/npm-run.py\n",
      "../scrape_github/electron/build/mac/make_locale_dirs.py\n"
     ]
    }
   ],
   "source": [
    "# dataset, dataset_edit = generate_datasets(opt)\n",
    "dataset = generate_datasets(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(x, y, n=opt.n_leftmost_tokens):\n",
    "    \"\"\"\n",
    "    Cut and paste the prefix of y at the end of x\n",
    "    \"\"\"\n",
    "    prefix_tokens = get_prefix(y, n=n)\n",
    "    tmp_prefix_ptr = '\\s*' + '\\s*'.join([re.escape(token) for token in prefix_tokens])\n",
    "    prefix_index = re.search(tmp_prefix_ptr, y).end()\n",
    "    x += '\\n' + y[:prefix_index]  # Valid only if each unit is separated by lines\n",
    "    y = y[prefix_index:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def reverse_process_example(x, y, n=opt.n_leftmost_tokens):\n",
    "    \"\"\"\n",
    "    Cut and paste the suffix of x at the beginning of y\n",
    "    \"\"\"\n",
    "    suffix_tokens = get_suffix(x, n=n)\n",
    "    tmp_suffix_ptr = '\\s*' + '\\s*'.join([re.escape(token) for token in suffix_tokens]) + '$'\n",
    "    prefix_index = re.search(tmp_suffix_ptr, x).start()\n",
    "    y = strip_empty_lines(x[prefix_index:] + y)\n",
    "    x = x[:prefix_index]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def extract_examples_line_by_line(source,\n",
    "                                  n=opt.n_leftmost_tokens,\n",
    "                                  min_len=opt.min_len):  # If line, no need to filter based on length\n",
    "    examples = []\n",
    "    def add_example(x, y):\n",
    "        x, y = process_example(x, y, n=n)\n",
    "        examples.append((x, y))\n",
    "\n",
    "    lines = get_lines_from_source(source,\n",
    "                                  remove_comments_from_source=True,\n",
    "                                  remove_empty_lines_from_source=True)\n",
    "    if not lines:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    add_example(x=\"\", y=lines[0])  # First example doesn't have context\n",
    "    for i in range(1, len(lines) - 1):\n",
    "        add_example(x=lines[i], y=lines[i + 1])\n",
    "\n",
    "    # Filter out examples that are too short\n",
    "    examples = [(x, y) for (x, y) in examples if len(x.strip()) > min_len and len(y.strip()) > min_len]\n",
    "\n",
    "    # Filter out examples that are import stmts\n",
    "#     examples = [(x, y) for (x, y) in examples if 'import' not in x and 'import' not in y]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hey = extract_examples_line_by_line(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('import sys\\nTEMPLATE',\n",
       " ' = \"\"\"\\n\\nnamespace node {{\\n\\nnamespace native_module {{\\n\\n{definitions}\\n\\nvoid NativeModuleLoader::LoadEmbedderJavaScriptSource() {{\\n  {initializers}\\n}}\\n\\n}}  // namespace native_module\\n\\n}}  // namespace node\\n\"\"\"')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hey[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Utils\n",
    "###############################################################################\n",
    "\n",
    "def rand_select(list, k):\n",
    "    print(f\"Randomly selected {k} examples from {len(list)} examples:\")\n",
    "    return random.choices(list, k=k)\n",
    "\n",
    "\n",
    "def strip_empty_lines(s):\n",
    "    \"\"\"\n",
    "    Remove empty lines at first and last.\n",
    "    \"\"\"\n",
    "    lines = s.splitlines()\n",
    "    while lines and not lines[0].strip():\n",
    "        lines.pop(0)\n",
    "    while lines and not lines[-1].strip():\n",
    "        lines.pop()\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def split_newlines(s):\n",
    "    \"\"\"\n",
    "    Split based on new lines (\\n) outside of quotes.\n",
    "\n",
    "    Note that this coalesces several newlines into one,\n",
    "    as blank lines are ignored. To avoid that, give a null case:\n",
    "\n",
    "    (?:\"[^\"]*\"|.)+|(?!\\Z)\n",
    "    \"\"\"\n",
    "    return re.findall(newline_ptr, s)\n",
    "\n",
    "\n",
    "def remove_null(l):\n",
    "    return list(filter(None, l))\n",
    "\n",
    "\n",
    "def remove_comments(source):\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", re.sub(comment_ptr, \"\", source))\n",
    "\n",
    "\n",
    "def remove_redundant_indentation(code):\n",
    "    lines = split_newlines(code)\n",
    "    redundant_indentation = min([len(line) - len(line.lstrip())\n",
    "                                 for line in lines\n",
    "                                 if len(line.strip()) > 0])\n",
    "    lines = [line[redundant_indentation:] for line in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def to_string_opt(opt):\n",
    "    \"\"\"\n",
    "    Generate a string for filename that contains current options\n",
    "\n",
    "    E.g. u_line__d_metric_n__d_thre_0.5__n_1__max_can_5\n",
    "    \"\"\"\n",
    "    s = []\n",
    "    s.append(f'u_{opt.unit}')\n",
    "    s.append(f'd_metric_x_{opt.distance_metric_x}')\n",
    "    s.append(f'd_thre_x_{opt.distance_threshold_x}')\n",
    "    s.append(f'd_metric_y_{opt.distance_metric_y}')\n",
    "    s.append(f'd_thre_y_{opt.distance_threshold_y}')\n",
    "    s.append(f'n_{opt.n_leftmost_tokens}')\n",
    "    s.append(f'max_can_{opt.max_num_candidates}')\n",
    "    return '__'.join(s)\n",
    "\n",
    "\n",
    "def get_lines_from_source(source,\n",
    "                          remove_comments_from_source,\n",
    "                          remove_empty_lines_from_source):\n",
    "    \"\"\"\n",
    "    Remove comments and empty lines from source.\n",
    "    Return a list of lines\n",
    "    \"\"\"\n",
    "    if remove_comments_from_source:\n",
    "        source = remove_comments(source)\n",
    "\n",
    "    lines = split_newlines(source)\n",
    "\n",
    "    if remove_empty_lines_from_source:\n",
    "        lines = [line for line in lines if len(line.strip()) > 0]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Tokenize\n",
    "###############################################################################\n",
    "\n",
    "def tokenize(s,\n",
    "             split_camelcase,\n",
    "             split_number_from_alpha,\n",
    "             keep_literal,\n",
    "             keep_whitespace,\n",
    "             verbose=False):\n",
    "\n",
    "    numbers, strings, delimiters = [], [], []\n",
    "\n",
    "    if keep_literal:\n",
    "        numbers = remove_null(set(re.findall(number_ptr, s)))\n",
    "        strings = remove_null(set(re.findall(string_ptr, s)))\n",
    "\n",
    "        literals = [f\"(?<=[^a-zA-Z0-9]){re.escape(l)}|^{re.escape(l)}\" for l in numbers]  # Add negative look ahead to exclude cases like fc1\n",
    "        literals.extend([re.escape(l) for l in strings])\n",
    "\n",
    "        delimiters = sorted(literals, key=len, reverse=True)\n",
    "\n",
    "    # Basic tokenization based on non alphanumeric tokens\n",
    "    delimiters.append(\"[^a-zA-Z0-9]\")  # Be careful of the order\n",
    "    delimiters = remove_null(delimiters)\n",
    "\n",
    "    tmp_code_ptr = \"({})\".format(\"|\".join(delimiters))\n",
    "    tokens = remove_null(re.split(tmp_code_ptr, s))\n",
    "    if verbose:\n",
    "        print('[tokenize] Basic:', tokens)\n",
    "\n",
    "    if split_camelcase:\n",
    "        before = tokens\n",
    "        tokens = []\n",
    "        for token in before:\n",
    "            if not token:\n",
    "                continue\n",
    "            elif token in numbers or token in strings:\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.extend(re.split(camelcase_ptr, token))\n",
    "        tokens = remove_null(tokens)\n",
    "        if verbose:\n",
    "            print('[tokenize] Split camel cases:', tokens)\n",
    "\n",
    "    if split_number_from_alpha:\n",
    "        before = tokens\n",
    "        tokens = []\n",
    "        for token in before:\n",
    "            if not token:\n",
    "                continue\n",
    "            elif token in numbers or token in strings:\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.extend(re.split(number_with_alpha_ptr, token))\n",
    "        tokens = remove_null(tokens)\n",
    "        if verbose:\n",
    "            print('[tokenize] Split numbers from alpha:', tokens)\n",
    "\n",
    "    if not keep_whitespace:\n",
    "        tokens = [token for token in tokens if len(token.strip()) > 0]\n",
    "        if verbose:\n",
    "            print('[tokenize] Remove whitespace:', tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_fine_grained(s, keep_whitespace=False):\n",
    "    \"\"\"\n",
    "    Tokenize as much as possible. Used when calculating edit distance.\n",
    "\n",
    "    E.g., camelCase45 = \"hi there\" -> camel, Case, 45, \", hi, there, \"\n",
    "    \"\"\"\n",
    "    return tokenize(s,\n",
    "                    split_camelcase=True,\n",
    "                    split_number_from_alpha=True,\n",
    "                    keep_literal=False,\n",
    "                    keep_whitespace=keep_whitespace)\n",
    "\n",
    "\n",
    "def tokenize_keywords(s):\n",
    "    \"\"\"\n",
    "    Tokenize as much as human-preferable. Used when generating keywords.\n",
    "\n",
    "    Do not tokenize based on literals.\n",
    "    By default, whitespace is entirely removed.\n",
    "    \"\"\"\n",
    "    return tokenize(s,\n",
    "                    split_camelcase=True,\n",
    "                    split_number_from_alpha=True,\n",
    "                    keep_literal=True,\n",
    "                    keep_whitespace=False)\n",
    "\n",
    "\n",
    "def get_prefix(y,\n",
    "               n=1):#opt.n_leftmost_tokens):\n",
    "    return tuple(tokenize_keywords(y)[:n])\n",
    "\n",
    "\n",
    "def get_suffix(x,\n",
    "               n=1):#opt.n_leftmost_tokens):\n",
    "    return tuple(tokenize_keywords(x)[-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Print stuff in color\n",
    "###############################################################################\n",
    "\n",
    "def print_example(example):\n",
    "    x, y = example\n",
    "    print(colored(x, 'red') + colored(y, 'blue'))\n",
    "\n",
    "\n",
    "def print_examples(examples):\n",
    "    print(\"-------------------------------\")\n",
    "    for example in examples:\n",
    "        print_example(example)\n",
    "        print(\"-------------------------------\")\n",
    "\n",
    "\n",
    "def print_candidates(example, candidates):\n",
    "    print('[x] -------------------------------')\n",
    "    print(colored(example[0], 'red') + colored(example[1], 'blue'))\n",
    "    for i, (edit_distance, x, y) in enumerate(candidates, 1):\n",
    "        print(f\"[{i}] {edit_distance:.2f} --------------------------\")\n",
    "        print_example((x, y))\n",
    "\n",
    "\n",
    "def print_example_edit(example_edit):\n",
    "    x, y_abs, x_prime, y_prime = example_edit\n",
    "    print_example((x_prime, y_prime))\n",
    "    print(colored('-------------------------------', 'white'))\n",
    "    print_example((x, y_abs))\n",
    "\n",
    "\n",
    "def print_dataset_edit(dataset_edit):\n",
    "    for i, example_edit in enumerate(dataset_edit, 1):\n",
    "        print(f\"[{i}] -------------------------------\")\n",
    "        print_example_edit(example_edit)\n",
    "\n",
    "\n",
    "def plot_histogram(array, title=\"\", xlabel=\"\", ylabel=\"\"):\n",
    "    n, bins, patches = plt.hist(array, bins=30, facecolor='g', alpha=0.75)\n",
    "    plt.grid(True)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Extract (x, y) pairs\n",
    "###############################################################################\n",
    "\n",
    "def process_example(x, y, n=opt.n_leftmost_tokens):\n",
    "    \"\"\"\n",
    "    Cut and paste the prefix of y at the end of x\n",
    "    \"\"\"\n",
    "    prefix_tokens = get_prefix(y, n=n)\n",
    "    tmp_prefix_ptr = '\\s*' + '\\s*'.join([re.escape(token) for token in prefix_tokens])\n",
    "    prefix_index = re.search(tmp_prefix_ptr, y).end()\n",
    "    x += '\\n' + y[:prefix_index]  # Valid only if each unit is separated by lines\n",
    "    y = y[prefix_index:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def reverse_process_example(x, y, n=opt.n_leftmost_tokens):\n",
    "    \"\"\"\n",
    "    Cut and paste the suffix of x at the beginning of y\n",
    "    \"\"\"\n",
    "    suffix_tokens = get_suffix(x, n=n)\n",
    "    tmp_suffix_ptr = '\\s*' + '\\s*'.join([re.escape(token) for token in suffix_tokens]) + '$'\n",
    "    prefix_index = re.search(tmp_suffix_ptr, x).start()\n",
    "    y = strip_empty_lines(x[prefix_index:] + y)\n",
    "    x = x[:prefix_index]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def extract_examples_line_by_line(source,\n",
    "                                  n=opt.n_leftmost_tokens,\n",
    "                                  min_len=opt.min_len):  # If line, no need to filter based on length\n",
    "    examples = []\n",
    "    def add_example(x, y):\n",
    "        x, y = process_example(x, y, n=n)\n",
    "        examples.append((x, y))\n",
    "\n",
    "    lines = get_lines_from_source(source,\n",
    "                                  remove_comments_from_source=True,\n",
    "                                  remove_empty_lines_from_source=True)\n",
    "    if not lines:\n",
    "        return []\n",
    "\n",
    "    add_example(x=\"\", y=lines[0])  # First example doesn't have context\n",
    "    for i in range(1, len(lines) - 1):\n",
    "        add_example(x=lines[i], y=lines[i + 1])\n",
    "\n",
    "    # Filter out examples that are too short\n",
    "    examples = [(x, y) for (x, y) in examples if len(x.strip()) > min_len and len(y.strip()) > min_len]\n",
    "\n",
    "    # Filter out examples that are import stmts\n",
    "    examples = [(x, y) for (x, y) in examples if 'import' not in x and 'import' not in y]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Calculate distance\n",
    "###############################################################################\n",
    "\n",
    "def abstract(s):\n",
    "    return re.sub(literal_ptr, ' ', s)\n",
    "\n",
    "\n",
    "def has_alpha(tokens):\n",
    "    return any([token.isalpha() for token in tokens])\n",
    "\n",
    "\n",
    "def iterative_levenshtein(s, t, costs=(1, 1, 1)):\n",
    "    \"\"\"\n",
    "        iterative_levenshtein(s, t) -> ldist\n",
    "        ldist is the Levenshtein distance between the strings\n",
    "        s and t.\n",
    "        For all i and j, dist[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "\n",
    "        costs: a tuple or a list with three integers (d, i, s)\n",
    "               where d defines the costs for a deletion\n",
    "                     i defines the costs for an insertion and\n",
    "                     s defines the costs for a substitution\n",
    "    \"\"\"\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    deletes, inserts, substitutes = costs\n",
    "\n",
    "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
    "    # source prefixes can be transformed into empty strings\n",
    "    # by deletions:\n",
    "    for row in range(1, rows):\n",
    "        dist[row][0] = row * deletes\n",
    "    # target prefixes can be created from an empty source string\n",
    "    # by inserting the characters\n",
    "    for col in range(1, cols):\n",
    "        dist[0][col] = col * inserts\n",
    "\n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = substitutes\n",
    "            dist[row][col] = min(dist[row-1][col] + deletes,\n",
    "                                 dist[row][col-1] + inserts,\n",
    "                                 dist[row-1][col-1] + cost) # substitution\n",
    "    return dist\n",
    "\n",
    "\n",
    "def backtrack_levenshtein(tokens1, tokens2, dist, verbose=False):\n",
    "    \"\"\"\n",
    "    Edit tokens2 to tokens1\n",
    "    \"\"\"\n",
    "    i = len(tokens1)\n",
    "    j = len(tokens2)\n",
    "\n",
    "    replaced_pairs = []  # list of (token2, token1) pairs\n",
    "    replaced_indices = [] # list of (index2, index1) pairs\n",
    "\n",
    "#     if verbose:\n",
    "#         for row in dist:\n",
    "#             print(row)\n",
    "#         print(tokens1)\n",
    "#         print(tokens2)\n",
    "\n",
    "    while i > 0 or j > 0:\n",
    "        if j <= 0:\n",
    "            if verbose:\n",
    "                print(\"Insert\", tokens1[i - 1])\n",
    "            i -= 1\n",
    "            continue\n",
    "        if i <= 0:\n",
    "            if verbose:\n",
    "                print(\"Delete\", tokens2[j - 1])\n",
    "            j -= 1\n",
    "            continue\n",
    "\n",
    "        if tokens1[i - 1] == tokens2[j - 1]:\n",
    "            if verbose:\n",
    "                print(f\"Same {tokens1[i - 1]} (i={i - 1}, j={j - 1})\")\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif j > 0 and dist[i][j] == dist[i][j - 1] + 1:\n",
    "            if verbose:\n",
    "                print(\"Delete\", tokens2[j - 1])\n",
    "            j -= 1\n",
    "        elif i > 0 and j > 0 and dist[i][j] == dist[i - 1][j - 1] + 1:\n",
    "            if verbose:\n",
    "                print(f\"Replace {tokens2[j - 1]} with {tokens1[i - 1]}\")\n",
    "            replaced_pairs.append((tokens2[j - 1], tokens1[i - 1]))\n",
    "            replaced_indices.append((j - 1, i - 1))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and dist[i][j] == dist[i - 1][j] + 1:\n",
    "            if verbose:\n",
    "                print(\"Insert\", tokens1[i - 1])\n",
    "            i -= 1\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"Error: i={i}, j={j}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return replaced_pairs, replaced_indices\n",
    "\n",
    "\n",
    "def collapse_edit_distance(tokens1, tokens2, verbose=False):\n",
    "    dist = iterative_levenshtein(tokens1, tokens2)\n",
    "    replaced_pairs, replaced_indices = backtrack_levenshtein(tokens1, tokens2, dist, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(replaced_pairs)\n",
    "\n",
    "    edit_distance = dist[-1][-1]\n",
    "    collapse = len(replaced_pairs) - len(set(replaced_pairs))  # Do not count same replacement multiple times\n",
    "    return edit_distance - collapse\n",
    "\n",
    "\n",
    "def calculate_edit_distance(code_block1,\n",
    "                            code_block2,\n",
    "                            ignore_literals,\n",
    "                            distance_metric,\n",
    "                            verbose=False):\n",
    "    if ignore_literals:  # Todo. Just ignore difference in strings if they are substentially different\n",
    "        block1 = abstract(code_block1)\n",
    "        block2 = abstract(code_block2)\n",
    "        if verbose:\n",
    "            print(\"[.] Abstracted code blocks:\")\n",
    "            print(block1.strip())\n",
    "            print(block2.strip())\n",
    "\n",
    "    else:\n",
    "        block1 = code_block1\n",
    "        block2 = code_block2\n",
    "\n",
    "    # Tokenize\n",
    "    tokens1 = tokenize_fine_grained(block1, keep_whitespace=False)\n",
    "    tokens2 = tokenize_fine_grained(block2, keep_whitespace=False)\n",
    "\n",
    "    if not tokens1 or not tokens2:\n",
    "        return float('inf')\n",
    "\n",
    "    if not has_alpha(tokens1) or not has_alpha(tokens2):\n",
    "        return float('inf')\n",
    "\n",
    "    if verbose:\n",
    "        print(tokens1)\n",
    "        print(tokens2)\n",
    "\n",
    "    # https://github.com/doukremt/distance\n",
    "    if distance_metric == \"j\":\n",
    "        return distance.jaccard(tokens1, tokens2)\n",
    "    elif distance_metric == \"l\":\n",
    "        return distance.levenshtein(tokens1, tokens2)\n",
    "    elif distance_metric == \"h\":\n",
    "        return distance.hamming(tokens1, tokens2)\n",
    "    elif distance_metric == \"s\":\n",
    "        return distance.sorensen(tokens1, tokens2)\n",
    "    elif distance_metric == \"n\":  # Normalized Levenshtein\n",
    "        return distance.nlevenshtein(tokens1, tokens2)\n",
    "    elif distance_metric == \"c\":  # Collapsed Levenshtein edit distance\n",
    "        return collapse_edit_distance(tokens1, tokens2, verbose=verbose)\n",
    "    elif distance_metric == \"nc\":  # Normalized collapsed Levenshtein edit distance\n",
    "        collapsed = collapse_edit_distance(tokens1, tokens2, verbose=verbose)\n",
    "        return collapsed / max(len(tokens1), len(tokens2))\n",
    "\n",
    "\n",
    "def replace_diff_with_placeholders(string1, string2):\n",
    "    \"\"\"\n",
    "    Replace string2-specific tokens with placeholders. Keep original values.\n",
    "\n",
    "    E.g. string1 = 'self.fc7 = (self.relu7, 4096, 4096, \"fc7\")'\n",
    "         string2 = 'self.fc8 = (self.relu8, 4096, 1000, \"fc8\")'\n",
    "\n",
    "         return 'self.fc[[8]] = (self.relu[[8]], 4096, [[1000]], \"fc[[8]]\")'\n",
    "\n",
    "    Parameters:\n",
    "        string1 (str): y\n",
    "        string2 (str): y'; to be abstracted to be consistent with y\n",
    "    \"\"\"\n",
    "    tokens1 = tokenize_fine_grained(string1, keep_whitespace=True)  # Need to keep whitespace\n",
    "    tokens2 = tokenize_fine_grained(string2, keep_whitespace=True)  # Need to keep whitespace\n",
    "\n",
    "    dist = iterative_levenshtein(tokens1, tokens2)\n",
    "    replaced_pairs, replaced_indices = backtrack_levenshtein(tokens1, tokens2, dist)\n",
    "\n",
    "    replaced = tokens2\n",
    "    for index2, index1 in replaced_indices:\n",
    "        replaced[index2] = f'[[{replaced[index2]}]]'\n",
    "    return ''.join(replaced)\n",
    "\n",
    "\n",
    "def rank_based_on_distance(opt,\n",
    "                           example,\n",
    "                           examples,\n",
    "                           include_unsatisfying_examples,\n",
    "                           exclude_same_context,\n",
    "                           exclude_same_example,\n",
    "                           n=opt.n_leftmost_tokens,\n",
    "                           check_exact_match_suffix=opt.check_exact_match_suffix,\n",
    "                           verbose=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        check_exact_match_suffix (bool): make sure that the suffix of x\n",
    "            (n_leftmost_tokens tokens) exactly matches\n",
    "        exclude_unsatisfying_examples (bool): filter out examples whose\n",
    "            edit distance is larger than distance_threshold\n",
    "        exclude_same_context (bool): exclude examples whose context exactly matches x\n",
    "    \"\"\"\n",
    "    ranked_examples = []\n",
    "    x, y = example\n",
    "    if verbose:\n",
    "        print(\"[..] x:\", x)\n",
    "\n",
    "    candidate_examples = examples\n",
    "    \n",
    "    # Select examples whose suffix of x exactly matches with that of example\n",
    "    if check_exact_match_suffix:\n",
    "        candidate_examples = []\n",
    "        x_suffix = get_suffix(x, n)\n",
    "        if verbose:\n",
    "            print(f\"[.] Enforce to have same {n} tokens as suffix: {x_suffix}\")\n",
    "        for x_prime, y_prime in examples:\n",
    "            x_prime_suffix = get_suffix(x_prime, n)\n",
    "            if verbose:\n",
    "                print(\"[..] x':\", x_prime)\n",
    "                print(\"[..] Rightmost tokens:\", x_prime_suffix, \"\\n\")\n",
    "            if x_suffix == x_prime_suffix:\n",
    "                candidate_examples.append((x_prime, y_prime))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[.] Ranking {len(candidate_examples)} examples\")\n",
    "    for x_prime, y_prime in candidate_examples:\n",
    "        edit_distance_x = calculate_edit_distance(x,\n",
    "                                                  x_prime,\n",
    "                                                  distance_metric=opt.distance_metric_x,\n",
    "                                                  ignore_literals=False)\n",
    "        edit_distance_y = calculate_edit_distance(y,\n",
    "                                                  y_prime,\n",
    "                                                  distance_metric=opt.distance_metric_y,\n",
    "                                                  ignore_literals=False)\n",
    "\n",
    "        if include_unsatisfying_examples:  # Include all examples\n",
    "            ranked_examples.append((edit_distance, x_prime, y_prime))\n",
    "        elif edit_distance_x <= opt.distance_threshold_x and edit_distance_y <= opt.distance_threshold_y:\n",
    "            if exclude_same_context and edit_distance_x == 0:\n",
    "                continue\n",
    "            if exclude_same_example and edit_distance_x == 0 and edit_distance_y == 0:\n",
    "                continue\n",
    "            ranked_examples.append((edit_distance_x, edit_distance_y, x_prime, y_prime))\n",
    "\n",
    "    return sorted(set(ranked_examples), key=lambda x:(x[1], x[0]))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Construct datasets\n",
    "###############################################################################\n",
    "\n",
    "def construct_examples_with_suffix(dataset):\n",
    "    \"\"\"\n",
    "    To speed up, construct clusters of examples based on their suffix\n",
    "    \"\"\"\n",
    "    examples_with_suffix = collections.defaultdict(list)\n",
    "    for example in dataset:\n",
    "        x = example[0]\n",
    "        x_suffix = get_suffix(x)\n",
    "        examples_with_suffix[x_suffix].append(example)\n",
    "    return examples_with_suffix\n",
    "\n",
    "\n",
    "def construct_dataset(sources):\n",
    "    \"\"\"\n",
    "    From a project, extract all (x, y) pairs\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    num_lines = 0\n",
    "    for source in sources:\n",
    "        num_lines += len(get_lines_from_source(source,\n",
    "                                               remove_comments_from_source=True,\n",
    "                                               remove_empty_lines_from_source=True))\n",
    "        examples = extract_examples_line_by_line(source)\n",
    "        dataset.extend(examples)\n",
    "\n",
    "    print(f\"\\n[construct_dataset] Number of lines in the project: {num_lines}\")\n",
    "    print(f\"[construct_dataset] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def construct_dataset_edit(opt,\n",
    "                           dataset,\n",
    "                           verbose=False):\n",
    "    \"\"\"\n",
    "    Given (x, y) pairs, generate (x, y_abs, x', y')\n",
    "    \"\"\"\n",
    "    dataset_edit = []\n",
    "    num_examples_with_candidates = 0\n",
    "    examples_with_suffix = construct_examples_with_suffix(dataset)\n",
    "\n",
    "    print(\"\\n[construct_dataset_edit] Start generating dataset for edit\")\n",
    "    for example in tqdm(dataset):\n",
    "        x, y = example\n",
    "        x_suffix = get_suffix(x)\n",
    "        candidates = rank_based_on_distance(opt,\n",
    "                                            example,\n",
    "                                            examples_with_suffix[x_suffix],\n",
    "                                            check_exact_match_suffix=False,  # No need to check if passing examples_with_suffix\n",
    "                                            include_unsatisfying_examples=False,  # Difference: filter out\n",
    "                                            exclude_same_context=False,\n",
    "                                            exclude_same_example=True,\n",
    "                                            verbose=verbose)\n",
    "        if not candidates:\n",
    "            pass\n",
    "        else:\n",
    "            num_examples_with_candidates += 1\n",
    "            for candidate in candidates[:opt.max_num_candidates]:\n",
    "                edit_distance_x, edit_distance_y, x_prime, y_prime = candidate\n",
    "                y_abs = y  # TODO\n",
    "                dataset_edit.append((x, y_abs, x_prime, y_prime))\n",
    "    print(f\"[construct_dataset_edit] Number of examples covered for edit: {num_examples_with_candidates}/{len(dataset)} ({num_examples_with_candidates/len(dataset)*100:.2f}%)\")\n",
    "    print(f\"[construct_dataset_edit] Number of examples for edit: {len(dataset_edit)} ({len(set(dataset_edit))} unique examples for edit)\")\n",
    "    return dataset_edit\n",
    "\n",
    "\n",
    "def generate_datasets(opt):\n",
    "    \"\"\"\n",
    "    Check if processed datasets exist.\n",
    "\n",
    "    If so, read. Otherwise, generate and save to the path.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[generate_datasets] Processing {opt.path}\")\n",
    "    options = to_string_opt(opt)\n",
    "    path_dataset = os.path.join(opt.path, f\"dataset_{options}.p\")\n",
    "    path_dataset_edit = os.path.join(opt.path, f\"dataset_edit_{options}.p\")\n",
    "\n",
    "    if os.path.exists(path_dataset) and os.path.exists(path_dataset_edit) and not opt.overwrite:\n",
    "        print(\"[generate_datasets] Read from pickled files\")\n",
    "        dataset = read_from_pickle(path_dataset)\n",
    "        dataset_edit = read_from_pickle(path_dataset_edit)\n",
    "    else:\n",
    "        sources, num_file = read_data(opt.path)\n",
    "\n",
    "        dataset = construct_dataset(sources)  # D_proj = {(x, y)}\n",
    "        return dataset\n",
    "        if len(dataset) > 10000:\n",
    "            print(f\"[generate_datasets] Skipping too large project (|dataset| = {len(dataset)})\")\n",
    "            dataset_edit = []\n",
    "        else:\n",
    "            dataset_edit = construct_dataset_edit(opt, dataset)  # D_edit = {(x, y, x', y')}\n",
    "\n",
    "            # Save datasets for future use\n",
    "            if opt.save:\n",
    "                save_as_pickle(path_dataset, dataset)\n",
    "                save_as_pickle(path_dataset_edit, dataset_edit)\n",
    "\n",
    "    print(f\"\\n[generate_datasets] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "    print(f\"[generate_datasets] Number of examples for edit: {len(dataset_edit)} ({len(set(dataset_edit))} unique examples)\\n\")\n",
    "\n",
    "    return dataset, dataset_edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Render html\n",
    "###############################################################################\n",
    "\n",
    "ignorable = '(\\s*\"\"\".*?\"\"\"\\s*|\\s*\\'\\'\\'.*?\\'\\'\\'\\s*|\\s)*'\n",
    "\n",
    "\n",
    "def generate_html(sources):\n",
    "    html = \"\"\n",
    "    for source in sources:\n",
    "        if len(source.strip()) == 0:\n",
    "            continue\n",
    "        html += f\"#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#\\n\\n{source}\\n\\n\"\n",
    "    return html\n",
    "\n",
    "\n",
    "def collect_code_with_edits(dataset_edit, html):\n",
    "    \"\"\"\n",
    "    Collect all exact code (~ x + y) that have example edits in dataset_edit.\n",
    "    \"\"\"\n",
    "    # TODO Ignore one liner comments starting with #\n",
    "    code_with_edits = set()\n",
    "    cnt_not_found, cnt_found = 0, 0\n",
    "    for x, y, x_prime, y_prime in dataset_edit:  # NOTE y is not abstracted\n",
    "        x, y = reverse_process_example(x, y)  # Cut and paste the suffix of x to the beginning of y\n",
    "        code_to_find_ptr = re.escape(x.strip()) + ignorable + re.escape(y.strip())  # Add ignorable\n",
    "        code_to_find_ptr = re.compile(code_to_find_ptr, re.MULTILINE|re.DOTALL)\n",
    "\n",
    "        # Find actual code from html\n",
    "        found = re.search(code_to_find_ptr, html)\n",
    "        if not found:\n",
    "            print(\"[!] code block not found in source code\")\n",
    "            print(x)\n",
    "            print(y)\n",
    "            cnt_not_found += 1\n",
    "        else:\n",
    "            code = found.group(0).strip()\n",
    "            code_with_edits.add(code)\n",
    "            cnt_found += 1\n",
    "    print(\"\\nMark code with edits in html\")\n",
    "    print(\"Found:\", cnt_found)\n",
    "    print(\"Not found:\", cnt_not_found)\n",
    "    return sorted(code_with_edits, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] Executing main function with options\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-38898812aceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Execute main function with default options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-38898812aceb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[main] Executing main function with options\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_edit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Main\n",
    "###############################################################################\n",
    "\n",
    "def main(opt):\n",
    "    print(\"[main] Executing main function with options\")\n",
    "    sources, num_files = read_data(opt.path)\n",
    "    dataset, dataset_edit = generate_datasets(opt)\n",
    "\n",
    "    html = generate_html(sources)  # For rendering\n",
    "    code_with_edits = collect_code_with_edits(dataset_edit, html)  # For rendering\n",
    "    examples_with_suffix = construct_examples_with_suffix(dataset)  # For metadata\n",
    "\n",
    "    print(\"\\nTotal number of files processed:\", num_files)\n",
    "\n",
    "    print(\"\\nDistance metric (x):\", opt.distance_metric_x)\n",
    "    print(\"Distance threshold (x):\", opt.distance_threshold_x)\n",
    "    print(\"Distance metric (y):\", opt.distance_metric_y)\n",
    "    print(\"Distance threshold (y):\", opt.distance_threshold_y)\n",
    "\n",
    "    # print(\"\\nNumber of leftmost tokens for keywords (n):\", opt.n_leftmost_tokens)\n",
    "    # print(\"Maximum number of candidates to generate example edits (k):\", opt.max_num_candidates)\n",
    "    # print(\"Enforce exact match of the suffix of x and x':\", opt.check_exact_match_suffix)\n",
    "\n",
    "    print(f\"\\nTotal number of examples: {len(dataset)}\")\n",
    "    print(f\"Total number of example edits: {len(dataset_edit)} ({len(dataset_edit) / len(dataset) * 100:.2f}%)\")\n",
    "    print(f\"Total number of code with edits: {len(code_with_edits)}\", \"\\n\")\n",
    "\n",
    "    return {\n",
    "        'sources': sources,\n",
    "        'num_files': num_files,\n",
    "        'dataset': dataset,\n",
    "        'dataset_edit': dataset_edit,\n",
    "        'html': html,\n",
    "        'code_with_edits': code_with_edits,\n",
    "        'examples_with_suffix': examples_with_suffix\n",
    "    }\n",
    "\n",
    "\n",
    "# Execute main function with default options\n",
    "results = main(opt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b9ae3311c9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/apply_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'app' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Flask\n",
    "###############################################################################\n",
    "\n",
    "@app.route(\"/apply_options\", methods=[\"POST\"])\n",
    "def apply_options():\n",
    "    options = request.get_json(force=True)\n",
    "    print(\"\\n[apply_options] Applying requested options:\", options)\n",
    "\n",
    "    # Update opt values\n",
    "    global opt\n",
    "    opt.distance_metric_x = options['distance_metric_x']\n",
    "    opt.distance_threshold_x = float(options['distance_threshold_x'])\n",
    "    opt.distance_metric_y = options['distance_metric_y']\n",
    "    opt.distance_threshold_y = float(options['distance_threshold_y'])\n",
    "\n",
    "    # Update processed data for new options\n",
    "    global results\n",
    "    results = main(opt)\n",
    "    print('HERERERERE')\n",
    "    print(results)\n",
    "    return render_template(\"index.html\",\n",
    "                           html=results['html'],\n",
    "                           dataset_size=len(results['dataset']),\n",
    "                           dataset_edit_size=len(results['dataset_edit']),\n",
    "                           coverage=f\"{len(results['dataset_edit']) / len(results['dataset']) * 100:.2f}\",\n",
    "                           distance_metric_x=opt.distance_metric_x,\n",
    "                           distance_threshold_x=opt.distance_threshold_x,\n",
    "                           distance_metric_y=opt.distance_metric_y,\n",
    "                           distance_threshold_y=opt.distance_threshold_y)\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/get_code_with_edits\", methods=[\"POST\"])\n",
    "def get_code_with_edits():\n",
    "    return jsonify(results['code_with_edits'])\n",
    "\n",
    "\n",
    "@app.route(\"/get_metadata\", methods=[\"POST\"])\n",
    "def get_metadata():\n",
    "    example = request.get_json(force=True)\n",
    "    print(example)\n",
    "    # Process example\n",
    "\n",
    "    x = example['x'].replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    y = example['y'].replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    # print(\"Pre POrcexx examples ??????\")\n",
    "    # print(x)\n",
    "    # print(y)\n",
    "    x, y = process_example(x, y)\n",
    "    # print(\"POrcexx examples ??????\")\n",
    "    # print(x)\n",
    "    # print(y)\n",
    "\n",
    "    # Get metadata\n",
    "    x_suffix = get_suffix(x)\n",
    "    print(\"about to process example edits\")\n",
    "    example_edits = rank_based_on_distance(opt,\n",
    "                                           (x, y),\n",
    "                                           results['examples_with_suffix'][x_suffix],\n",
    "                                           check_exact_match_suffix=False,  # No need to check if passing examples_with_suffix\n",
    "                                           include_unsatisfying_examples=False,\n",
    "                                           exclude_same_context=False,\n",
    "                                           exclude_same_example=True)\n",
    "    print(example_edits)\n",
    "    print(\"in between example edits\")\n",
    "    example_edits = [{\n",
    "                        'edit_distance_x': f'{edit_distance_x:.2f}',\n",
    "                        'edit_distance_y': f'{edit_distance_y:.2f}',\n",
    "                        'x': x,\n",
    "                        'y': y,\n",
    "                    }\n",
    "                    for (edit_distance_x, edit_distance_y, x, y) in example_edits]\n",
    "\n",
    "    data = {\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'example_edits': example_edits,\n",
    "    }\n",
    "    return jsonify(data)\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def intro():\n",
    "    return render_template(\"index.html\",\n",
    "                           html=results['html'],\n",
    "                           dataset_size=len(results['dataset']),\n",
    "                           dataset_edit_size=len(results['dataset_edit']),\n",
    "                           coverage=f\"{len(results['dataset_edit']) / len(results['dataset']) * 100:.2f}\",\n",
    "                           distance_metric_x=opt.distance_metric_x,\n",
    "                           distance_threshold_x=opt.distance_threshold_x,\n",
    "                           distance_metric_y=opt.distance_metric_y,\n",
    "                           distance_threshold_y=opt.distance_threshold_y,\n",
    "                           min_len=opt.min_len)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\",  # Public\n",
    "            debug=opt.debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
