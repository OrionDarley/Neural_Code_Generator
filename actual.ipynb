{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import distance\n",
    "import collections\n",
    "import configargparse\n",
    "from tqdm import tqdm\n",
    "from  more_itertools import unique_everseen\n",
    "\n",
    "# import opts\n",
    "\n",
    "from flask import Flask, render_template, url_for, redirect, jsonify, request  # noqa\n",
    "from flask_cors import CORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from': 0,\n",
       " '_': 1,\n",
       " 'future': 2,\n",
       " 'import': 3,\n",
       " 'print': 4,\n",
       " 'function': 5,\n",
       " 'os': 6,\n",
       " '.': 7,\n",
       " 'path': 8,\n",
       " 'warnings': 9,\n",
       " 'sys': 10,\n",
       " 'try': 11,\n",
       " ':': 12,\n",
       " 'setuptools': 13,\n",
       " 'setup': 14,\n",
       " ',': 15,\n",
       " 'Command': 16,\n",
       " 'available': 17,\n",
       " '=': 18,\n",
       " 'True': 19,\n",
       " 'except': 20,\n",
       " 'Import': 21,\n",
       " 'Error': 22,\n",
       " 'distutils': 23,\n",
       " 'core': 24,\n",
       " 'False': 25,\n",
       " 'spawn': 26,\n",
       " 'py': 27,\n",
       " '2': 28,\n",
       " 'exe': 29,\n",
       " 'if': 30,\n",
       " 'len': 31,\n",
       " '(': 32,\n",
       " 'argv': 33,\n",
       " ')': 34,\n",
       " '>': 35,\n",
       " 'and': 36,\n",
       " '[': 37,\n",
       " '1': 38,\n",
       " ']': 39,\n",
       " \"'\": 40,\n",
       " 'Cannot': 41,\n",
       " 'file': 42,\n",
       " 'stderr': 43,\n",
       " 'exit': 44,\n",
       " 'options': 45,\n",
       " '{': 46,\n",
       " 'bundle': 47,\n",
       " 'files': 48,\n",
       " 'compressed': 49,\n",
       " 'optimize': 50,\n",
       " 'dist': 51,\n",
       " 'dir': 52,\n",
       " 'dll': 53,\n",
       " 'excludes': 54,\n",
       " 'w': 55,\n",
       " '9': 56,\n",
       " 'xpopen': 57,\n",
       " 'crypt': 58,\n",
       " '32': 59,\n",
       " '}': 60,\n",
       " 'exec': 61,\n",
       " 'compile': 62,\n",
       " 'open': 63,\n",
       " 'youtube': 64,\n",
       " 'dl': 65,\n",
       " '/': 66,\n",
       " 'version': 67,\n",
       " 'read': 68,\n",
       " 'DESCRIPTION': 69,\n",
       " 'You': 70,\n",
       " 'Tube': 71,\n",
       " 'video': 72,\n",
       " 'downloader': 73,\n",
       " 'LONG': 74,\n",
       " '-': 75,\n",
       " 'line': 76,\n",
       " 'program': 77,\n",
       " 'to': 78,\n",
       " 'download': 79,\n",
       " 'videos': 80,\n",
       " 'com': 81,\n",
       " 'other': 82,\n",
       " 'sites': 83,\n",
       " 'console': 84,\n",
       " 'script': 85,\n",
       " 'main': 86,\n",
       " 'dest': 87,\n",
       " 'base': 88,\n",
       " 'description': 89,\n",
       " 'comments': 90,\n",
       " 'product': 91,\n",
       " 'name': 92,\n",
       " 'params': 93,\n",
       " 'zipfile': 94,\n",
       " 'None': 95,\n",
       " 'else': 96,\n",
       " 'spec': 97,\n",
       " 'etc': 98,\n",
       " 'bash': 99,\n",
       " 'completion': 100,\n",
       " 'd': 101,\n",
       " 'fish': 102,\n",
       " 'completions': 103,\n",
       " 'share': 104,\n",
       " 'doc': 105,\n",
       " 'README': 106,\n",
       " 'txt': 107,\n",
       " 'man': 108,\n",
       " 'root': 109,\n",
       " 'dirname': 110,\n",
       " 'abspath': 111,\n",
       " 'data': 112,\n",
       " 'for': 113,\n",
       " 'in': 114,\n",
       " 'resfiles': 115,\n",
       " 'fn': 116,\n",
       " 'not': 117,\n",
       " 'exists': 118,\n",
       " 'warn': 119,\n",
       " 'Skipping': 120,\n",
       " '%': 121,\n",
       " 's': 122,\n",
       " 'since': 123,\n",
       " 'it': 124,\n",
       " 'is': 125,\n",
       " 'present': 126,\n",
       " 'Type': 127,\n",
       " 'make': 128,\n",
       " 'build': 129,\n",
       " 'all': 130,\n",
       " 'automatically': 131,\n",
       " 'generated': 132,\n",
       " 'append': 133,\n",
       " 'entry': 134,\n",
       " 'points': 135,\n",
       " 'scripts': 136,\n",
       " 'bin': 137,\n",
       " 'class': 138,\n",
       " 'lazy': 139,\n",
       " 'extractors': 140,\n",
       " 'Build': 141,\n",
       " 'the': 142,\n",
       " 'extractor': 143,\n",
       " 'loading': 144,\n",
       " 'module': 145,\n",
       " 'user': 146,\n",
       " 'def': 147,\n",
       " 'initialize': 148,\n",
       " 'self': 149,\n",
       " 'pass': 150,\n",
       " 'finalize': 151,\n",
       " 'run': 152,\n",
       " 'executable': 153,\n",
       " 'devscripts': 154,\n",
       " 'dry': 155,\n",
       " 'long': 156,\n",
       " 'url': 157,\n",
       " 'https': 158,\n",
       " 'github': 159,\n",
       " 'ytdl': 160,\n",
       " 'org': 161,\n",
       " 'author': 162,\n",
       " 'Ricardo': 163,\n",
       " 'Garcia': 164,\n",
       " 'email': 165,\n",
       " '@': 166,\n",
       " 'yt': 167,\n",
       " 'maintainer': 168,\n",
       " 'Sergey': 169,\n",
       " 'M': 170,\n",
       " 'dstftw': 171,\n",
       " 'gmail': 172,\n",
       " 'license': 173,\n",
       " 'Unlicense': 174,\n",
       " 'packages': 175,\n",
       " 'postprocessor': 176,\n",
       " 'classifiers': 177,\n",
       " 'Topic': 178,\n",
       " 'Multimedia': 179,\n",
       " 'Video': 180,\n",
       " 'Development': 181,\n",
       " 'Status': 182,\n",
       " '5': 183,\n",
       " 'Production': 184,\n",
       " 'Stable': 185,\n",
       " 'Environment': 186,\n",
       " 'Console': 187,\n",
       " 'License': 188,\n",
       " 'Public': 189,\n",
       " 'Domain': 190,\n",
       " 'Programming': 191,\n",
       " 'Language': 192,\n",
       " 'Python': 193,\n",
       " '6': 194,\n",
       " '7': 195,\n",
       " '3': 196,\n",
       " '4': 197,\n",
       " '8': 198,\n",
       " 'Implementation': 199,\n",
       " 'CPython': 200,\n",
       " 'Iron': 201,\n",
       " 'Jython': 202,\n",
       " 'Py': 203,\n",
       " 'cmdclass': 204,\n",
       " '*': 205,\n",
       " 'unicode': 206,\n",
       " 'literals': 207,\n",
       " 'unittest': 208,\n",
       " 'insert': 209,\n",
       " '0': 210,\n",
       " 'test': 211,\n",
       " 'helper': 212,\n",
       " 'expect': 213,\n",
       " 'value': 214,\n",
       " 'Youtube': 215,\n",
       " 'IE': 216,\n",
       " 'Test': 217,\n",
       " 'Chapters': 218,\n",
       " 'Case': 219,\n",
       " 'TEST': 220,\n",
       " 'CASES': 221,\n",
       " '1477': 222,\n",
       " 'start': 223,\n",
       " 'time': 224,\n",
       " '36': 225,\n",
       " 'end': 226,\n",
       " '162': 227,\n",
       " 'title': 228,\n",
       " 'Bohemian': 229,\n",
       " 'Rhapsody': 230,\n",
       " '413': 231,\n",
       " 'Radio': 232,\n",
       " 'Ga': 233,\n",
       " '454': 234,\n",
       " 'Ay': 235,\n",
       " 'Oh': 236,\n",
       " '!': 237,\n",
       " '728': 238,\n",
       " 'Hammer': 239,\n",
       " 'To': 240,\n",
       " 'Fall': 241,\n",
       " '963': 242,\n",
       " 'Crazy': 243,\n",
       " 'Little': 244,\n",
       " 'Thing': 245,\n",
       " 'Called': 246,\n",
       " 'Love': 247,\n",
       " '1038': 248,\n",
       " 'We': 249,\n",
       " 'Will': 250,\n",
       " 'Rock': 251,\n",
       " '1272': 252,\n",
       " 'Are': 253,\n",
       " 'The': 254,\n",
       " 'Champions': 255,\n",
       " 'Is': 256,\n",
       " 'This': 257,\n",
       " 'World': 258,\n",
       " 'Created': 259,\n",
       " '?': 260,\n",
       " 'Those': 261,\n",
       " 'Beaten': 262,\n",
       " 'Paths': 263,\n",
       " 'of': 264,\n",
       " 'Confusion': 265,\n",
       " '<': 266,\n",
       " 'a': 267,\n",
       " 'href': 268,\n",
       " '\"': 269,\n",
       " 'onclick': 270,\n",
       " 'www': 271,\n",
       " 'watch': 272,\n",
       " 'player': 273,\n",
       " 'seek': 274,\n",
       " '60': 275,\n",
       " '+': 276,\n",
       " '00': 277,\n",
       " ';': 278,\n",
       " 'return': 279,\n",
       " 'false': 280,\n",
       " 'br': 281,\n",
       " 'Beyond': 282,\n",
       " 'Shadows': 283,\n",
       " 'Emptiness': 284,\n",
       " '&': 285,\n",
       " 'Nothingness': 286,\n",
       " '11': 287,\n",
       " '47': 288,\n",
       " 'Poison': 289,\n",
       " 'Yourself': 290,\n",
       " 'With': 291,\n",
       " 'Thought': 292,\n",
       " '26': 293,\n",
       " '30': 294,\n",
       " 'Agents': 295,\n",
       " 'Transformation': 296,\n",
       " '35': 297,\n",
       " '57': 298,\n",
       " 'Drowning': 299,\n",
       " 'Pain': 300,\n",
       " 'Consciousness': 301,\n",
       " '44': 302,\n",
       " 'Deny': 303,\n",
       " 'Disease': 304,\n",
       " 'Life': 305,\n",
       " '53': 306,\n",
       " '07': 307,\n",
       " 'More': 308,\n",
       " 'info': 309,\n",
       " 'Buy': 310,\n",
       " 'http': 311,\n",
       " 'crepusculonegro': 312,\n",
       " 'storenvy': 313,\n",
       " 'products': 314,\n",
       " '257645': 315,\n",
       " 'cn': 316,\n",
       " '03': 317,\n",
       " 'arizmenda': 318,\n",
       " 'within': 319,\n",
       " 'vacuum': 320,\n",
       " 'infinity': 321,\n",
       " 'No': 322,\n",
       " 'copyright': 323,\n",
       " 'intended': 324,\n",
       " 'rights': 325,\n",
       " 'this': 326,\n",
       " 'are': 327,\n",
       " 'assumed': 328,\n",
       " 'by': 329,\n",
       " 'owner': 330,\n",
       " 'its': 331,\n",
       " 'affiliates': 332,\n",
       " '4009': 333,\n",
       " '707': 334,\n",
       " '1590': 335,\n",
       " '2157': 336,\n",
       " '2672': 337,\n",
       " '3187': 338,\n",
       " 'aes': 339,\n",
       " 'decrypt': 340,\n",
       " 'encrypt': 341,\n",
       " 'cbc': 342,\n",
       " 'text': 343,\n",
       " 'utils': 344,\n",
       " 'bytes': 345,\n",
       " 'intlist': 346,\n",
       " '64': 347,\n",
       " 'AES': 348,\n",
       " 'set': 349,\n",
       " 'Up': 350,\n",
       " 'key': 351,\n",
       " 'iv': 352,\n",
       " '0x': 353,\n",
       " '20': 354,\n",
       " '15': 355,\n",
       " '14': 356,\n",
       " 'secret': 357,\n",
       " 'msg': 358,\n",
       " 'b': 359,\n",
       " 'Secret': 360,\n",
       " 'message': 361,\n",
       " 'goes': 362,\n",
       " 'here': 363,\n",
       " 'list': 364,\n",
       " 'range': 365,\n",
       " '16': 366,\n",
       " 'encrypted': 367,\n",
       " 'decrypted': 368,\n",
       " 'assert': 369,\n",
       " 'Equal': 370,\n",
       " '\\\\': 371,\n",
       " 'x': 372,\n",
       " '97': 373,\n",
       " '92': 374,\n",
       " 'xe': 375,\n",
       " 'xc': 376,\n",
       " '18': 377,\n",
       " '91': 378,\n",
       " 'ky': 379,\n",
       " 'm': 380,\n",
       " 'xb': 381,\n",
       " '96': 382,\n",
       " 'u': 383,\n",
       " '88': 384,\n",
       " 'xab': 385,\n",
       " 'e': 386,\n",
       " '|': 387,\n",
       " 'xf': 388,\n",
       " 'xcd': 389,\n",
       " 'bandcamp': 390,\n",
       " 'merch': 391,\n",
       " 'despairs': 392,\n",
       " 'depths': 393,\n",
       " 'descended': 394,\n",
       " 'cd': 395,\n",
       " 'uix': 396,\n",
       " 'servicelink': 397,\n",
       " 'target': 398,\n",
       " 'new': 399,\n",
       " 'window': 400,\n",
       " 'CDAQ': 401,\n",
       " 'Tg': 402,\n",
       " 'YACITCNf': 403,\n",
       " '1raq': 404,\n",
       " 'T': 405,\n",
       " '2d': 406,\n",
       " 'MCFd': 407,\n",
       " 'Rj': 408,\n",
       " 'GAod': 409,\n",
       " 'o': 410,\n",
       " 'CBSj': 411,\n",
       " 'HQ': 412,\n",
       " 'rel': 413,\n",
       " 'nofollow': 414,\n",
       " 'noopener': 415,\n",
       " 'blank': 416,\n",
       " 'Christening': 417,\n",
       " 'Unborn': 418,\n",
       " 'Deformities': 419,\n",
       " '08': 420,\n",
       " 'Taste': 421,\n",
       " 'Purity': 422,\n",
       " 'Sculpting': 423,\n",
       " 'Sins': 424,\n",
       " 'Universal': 425,\n",
       " 'Tongue': 426,\n",
       " '24': 427,\n",
       " '45': 428,\n",
       " 'Birth': 429,\n",
       " '31': 430,\n",
       " 'Neves': 431,\n",
       " '37': 432,\n",
       " '55': 433,\n",
       " 'Libations': 434,\n",
       " 'Limbo': 435,\n",
       " '2705': 436,\n",
       " '428': 437,\n",
       " '976': 438,\n",
       " 'rstrip': 439,\n",
       " 'password': 440,\n",
       " 'decode': 441,\n",
       " 'utf': 442,\n",
       " 'encode': 443,\n",
       " '17': 444,\n",
       " '93': 445,\n",
       " '80': 446,\n",
       " 'V': 447,\n",
       " 't': 448,\n",
       " 'xcdo': 449,\n",
       " 'xa': 450,\n",
       " 'xd': 451,\n",
       " 'ks': 452,\n",
       " 'r': 453,\n",
       " '27': 454,\n",
       " 'N': 455,\n",
       " 'xae': 456,\n",
       " '1485': 457,\n",
       " '1884': 458,\n",
       " '2275': 459,\n",
       " 'z': 460,\n",
       " 'i': 461,\n",
       " '85': 462,\n",
       " '99': 463,\n",
       " '98': 464,\n",
       " 'xbf': 465,\n",
       " '83': 466,\n",
       " 'Metadata': 467,\n",
       " 'From': 468,\n",
       " 'Title': 469,\n",
       " 'PP': 470,\n",
       " 'format': 471,\n",
       " 'regex': 472,\n",
       " 'pp': 473,\n",
       " 'artist': 474,\n",
       " 'titleregex': 475,\n",
       " 'P': 476,\n",
       " 'Download': 477,\n",
       " 'show': 478,\n",
       " 'MP': 479,\n",
       " 'sh': 480,\n",
       " 'st': 481,\n",
       " 'nj': 482,\n",
       " 'ZKK': 483,\n",
       " 'YACITCK': 484,\n",
       " '3j': 485,\n",
       " '6o': 486,\n",
       " 'MCFVDCGAodu': 487,\n",
       " 'VAKKij': 488,\n",
       " '4HQ': 489,\n",
       " 'Setlist': 490,\n",
       " 'I': 491,\n",
       " 'E': 492,\n",
       " 'A': 493,\n",
       " 'O': 494,\n",
       " 'Suite': 495,\n",
       " 'Pee': 496,\n",
       " 'Incomplete': 497,\n",
       " 'Attack': 498,\n",
       " 'First': 499,\n",
       " 'live': 500,\n",
       " 'performance': 501,\n",
       " '2011': 502,\n",
       " 'Prison': 503,\n",
       " 'Song': 504,\n",
       " '42': 505,\n",
       " 'Know': 506,\n",
       " '12': 507,\n",
       " 'Aerials': 508,\n",
       " 'Soldier': 509,\n",
       " 'Side': 510,\n",
       " 'Intro': 511,\n",
       " '19': 512,\n",
       " '13': 513,\n",
       " 'B': 514,\n",
       " 'Y': 515,\n",
       " '09': 516,\n",
       " 'Soil': 517,\n",
       " 'Darts': 518,\n",
       " '48': 519,\n",
       " '38': 520,\n",
       " 'Hypnotize': 521,\n",
       " '05': 522,\n",
       " 'Temper': 523,\n",
       " '1999': 524,\n",
       " 'CUBErt': 525,\n",
       " '41': 526,\n",
       " 'Needles': 527,\n",
       " 'Deer': 528,\n",
       " 'Dance': 529,\n",
       " '46': 530,\n",
       " 'Bounce': 531,\n",
       " '49': 532,\n",
       " 'Suggestions': 533,\n",
       " '51': 534,\n",
       " '25': 535,\n",
       " 'Psycho': 536,\n",
       " '52': 537,\n",
       " 'Chop': 538,\n",
       " 'Suey': 539,\n",
       " '58': 540,\n",
       " 'Lonely': 541,\n",
       " 'Day': 542,\n",
       " '3600': 543,\n",
       " '01': 544,\n",
       " 'Question': 545,\n",
       " '04': 546,\n",
       " 'Lost': 547,\n",
       " 'Hollywood': 548,\n",
       " '10': 549,\n",
       " 'Vicinity': 550,\n",
       " 'Obscenity': 551,\n",
       " '40': 552,\n",
       " '2012': 553,\n",
       " 'Forest': 554,\n",
       " 'Cigaro': 555,\n",
       " '02': 556,\n",
       " 'Toxicity': 557,\n",
       " '23': 558,\n",
       " 'with': 559,\n",
       " 'Chino': 560,\n",
       " 'Moreno': 561,\n",
       " 'Sugar': 562,\n",
       " '5640': 563,\n",
       " '266': 564,\n",
       " '331': 565,\n",
       " '522': 566,\n",
       " '752': 567,\n",
       " '932': 568,\n",
       " '1153': 569,\n",
       " '1209': 570,\n",
       " '1472': 571,\n",
       " '1668': 572,\n",
       " '1838': 573,\n",
       " '2105': 574,\n",
       " '2288': 575,\n",
       " '2460': 576,\n",
       " '2577': 577,\n",
       " '2787': 578,\n",
       " '2978': 579,\n",
       " '3085': 580,\n",
       " '3232': 581,\n",
       " '3493': 582,\n",
       " '3675': 583,\n",
       " '3854': 584,\n",
       " '4090': 585,\n",
       " '4420': 586,\n",
       " '4577': 587,\n",
       " '4802': 588,\n",
       " '5037': 589,\n",
       " '5273': 590,\n",
       " '1138': 591,\n",
       " '490': 592,\n",
       " 'В': 593,\n",
       " 'о': 594,\n",
       " 'п': 595,\n",
       " 'р': 596,\n",
       " 'а': 597,\n",
       " 'х': 598,\n",
       " 'Vo': 599,\n",
       " 'prakh': 600,\n",
       " '870': 601,\n",
       " 'И': 602,\n",
       " 'с': 603,\n",
       " 'к': 604,\n",
       " 'у': 605,\n",
       " 'л': 606,\n",
       " 'е': 607,\n",
       " 'н': 608,\n",
       " 'и': 609,\n",
       " 'Iskupleniye': 610,\n",
       " 'io': 611,\n",
       " 'json': 612,\n",
       " 'xml': 613,\n",
       " 'etree': 614,\n",
       " 'Element': 615,\n",
       " 'Tree': 616,\n",
       " 'age': 617,\n",
       " 'restricted': 618,\n",
       " 'args': 619,\n",
       " 'str': 620,\n",
       " 'n': 621,\n",
       " 'caesar': 622,\n",
       " 'clean': 623,\n",
       " 'html': 624,\n",
       " 'date': 625,\n",
       " 'Date': 626,\n",
       " 'Range': 627,\n",
       " 'detect': 628,\n",
       " 'determine': 629,\n",
       " 'ext': 630,\n",
       " 'dict': 631,\n",
       " 'get': 632,\n",
       " 'compat': 633,\n",
       " 'Filename': 634,\n",
       " 'escape': 635,\n",
       " 'rfc': 636,\n",
       " '3986': 637,\n",
       " 'extract': 638,\n",
       " 'attributes': 639,\n",
       " 'Extractor': 640,\n",
       " 'find': 641,\n",
       " 'xpath': 642,\n",
       " 'attr': 643,\n",
       " 'fix': 644,\n",
       " 'ampersands': 645,\n",
       " 'float': 646,\n",
       " 'or': 647,\n",
       " 'none': 648,\n",
       " 'element': 649,\n",
       " 'attribute': 650,\n",
       " 'elements': 651,\n",
       " 'In': 652,\n",
       " 'Advance': 653,\n",
       " 'Paged': 654,\n",
       " 'List': 655,\n",
       " 'int': 656,\n",
       " 'js': 657,\n",
       " 'limit': 658,\n",
       " 'length': 659,\n",
       " 'merge': 660,\n",
       " 'dicts': 661,\n",
       " 'mimetype': 662,\n",
       " 'month': 663,\n",
       " 'multipart': 664,\n",
       " 'ohdave': 665,\n",
       " 'rsa': 666,\n",
       " 'On': 667,\n",
       " 'Demand': 668,\n",
       " 'ordered': 669,\n",
       " 'Set': 670,\n",
       " 'parse': 671,\n",
       " 'з': 672,\n",
       " 'в': 673,\n",
       " 'ы': 674,\n",
       " 'Iz': 675,\n",
       " 'serpov': 676,\n",
       " 'luny': 677,\n",
       " '283': 678,\n",
       " 'chapters': 679,\n",
       " 'duration': 680,\n",
       " 'expected': 681,\n",
       " 'ie': 682,\n",
       " 'filesize': 683,\n",
       " 'count': 684,\n",
       " 'iso': 685,\n",
       " '8601': 686,\n",
       " 'resolution': 687,\n",
       " 'bitrate': 688,\n",
       " 'pkcs': 689,\n",
       " 'pad': 690,\n",
       " 'batch': 691,\n",
       " 'urls': 692,\n",
       " 'sanitize': 693,\n",
       " 'filename': 694,\n",
       " 'expand': 695,\n",
       " 'prepend': 696,\n",
       " 'extension': 697,\n",
       " 'replace': 698,\n",
       " 'remove': 699,\n",
       " 'quotes': 700,\n",
       " 'rot': 701,\n",
       " 'shell': 702,\n",
       " 'quote': 703,\n",
       " 'smuggle': 704,\n",
       " 'strip': 705,\n",
       " 'jsonp': 706,\n",
       " 'subtitles': 707,\n",
       " 'timeconvert': 708,\n",
       " 'unescape': 709,\n",
       " 'HTML': 710,\n",
       " 'unified': 711,\n",
       " 'strdate': 712,\n",
       " 'timestamp': 713,\n",
       " 'unsmuggle': 714,\n",
       " 'uppercase': 715,\n",
       " 'lowercase': 716,\n",
       " 'basename': 717,\n",
       " 'urljoin': 718,\n",
       " 'urlencode': 719,\n",
       " 'postdata': 720,\n",
       " 'urshift': 721,\n",
       " 'update': 722,\n",
       " 'query': 723,\n",
       " 'tuple': 724,\n",
       " 'ns': 725,\n",
       " 'render': 726,\n",
       " 'table': 727,\n",
       " 'match': 728,\n",
       " 'dfxp': 729,\n",
       " 'expr': 730,\n",
       " 'srt': 731,\n",
       " 'cli': 732,\n",
       " 'option': 733,\n",
       " 'valueless': 734,\n",
       " 'bool': 735,\n",
       " 'codecs': 736,\n",
       " 'chr': 737,\n",
       " 'fromstring': 738,\n",
       " 'getenv': 739,\n",
       " 'setenv': 740,\n",
       " 'urlparse': 741,\n",
       " 'qs': 742,\n",
       " 'Util': 743,\n",
       " 'bougrg': 744,\n",
       " 'abc': 745,\n",
       " '123': 746,\n",
       " 'de': 747,\n",
       " 'xxx': 748,\n",
       " 'yes': 749,\n",
       " 'no': 750,\n",
       " 'that': 751,\n",
       " 'AT': 752,\n",
       " 'aumlaut': 753,\n",
       " 'ä': 754,\n",
       " 'tests': 755,\n",
       " '043': 756,\n",
       " '0438': 757,\n",
       " '0440': 758,\n",
       " '0446': 759,\n",
       " '0430': 760,\n",
       " 'New': 761,\n",
       " 'record': 762,\n",
       " 'at': 763,\n",
       " '34': 764,\n",
       " 'gasdgf': 765,\n",
       " 'id': 766,\n",
       " 'forbidden': 767,\n",
       " 'fc': 768,\n",
       " 'fbc': 769,\n",
       " '56': 770,\n",
       " 'fd': 771,\n",
       " '7684': 772,\n",
       " 'c': 773,\n",
       " 'aab': 774,\n",
       " '$': 775,\n",
       " '`': 776,\n",
       " '^': 777,\n",
       " '5927': 778,\n",
       " 'f': 779,\n",
       " '603': 780,\n",
       " 'edf': 781,\n",
       " 'Speech': 782,\n",
       " 'Â': 783,\n",
       " 'Ã': 784,\n",
       " 'Ä': 785,\n",
       " 'À': 786,\n",
       " 'Á': 787,\n",
       " 'Å': 788,\n",
       " 'Æ': 789,\n",
       " 'Ç': 790,\n",
       " 'È': 791,\n",
       " 'É': 792,\n",
       " 'Ê': 793,\n",
       " 'Ë': 794,\n",
       " 'Ì': 795,\n",
       " 'Í': 796,\n",
       " 'Î': 797,\n",
       " 'Ï': 798,\n",
       " 'Ð': 799,\n",
       " 'Ñ': 800,\n",
       " 'Ò': 801,\n",
       " 'Ó': 802,\n",
       " 'Ô': 803,\n",
       " 'Õ': 804,\n",
       " 'Ö': 805,\n",
       " 'Ő': 806,\n",
       " 'Ø': 807,\n",
       " 'Œ': 808,\n",
       " 'Ù': 809,\n",
       " 'Ú': 810,\n",
       " 'Û': 811,\n",
       " 'Ü': 812,\n",
       " 'Ű': 813,\n",
       " 'Ý': 814,\n",
       " 'Þ': 815,\n",
       " 'ß': 816,\n",
       " 'à': 817,\n",
       " 'á': 818,\n",
       " 'â': 819,\n",
       " 'ã': 820,\n",
       " 'å': 821,\n",
       " 'æ': 822,\n",
       " 'ç': 823,\n",
       " 'è': 824,\n",
       " 'é': 825,\n",
       " 'ê': 826,\n",
       " 'ë': 827,\n",
       " 'ì': 828,\n",
       " 'í': 829,\n",
       " 'î': 830,\n",
       " 'ï': 831,\n",
       " 'ð': 832,\n",
       " 'ñ': 833,\n",
       " 'ò': 834,\n",
       " 'ó': 835,\n",
       " 'ô': 836,\n",
       " 'õ': 837,\n",
       " 'ö': 838,\n",
       " 'ő': 839,\n",
       " 'ø': 840,\n",
       " 'œ': 841,\n",
       " 'ù': 842,\n",
       " 'ú': 843,\n",
       " 'û': 844,\n",
       " 'ü': 845,\n",
       " 'ű': 846,\n",
       " 'ý': 847,\n",
       " 'þ': 848,\n",
       " 'ÿ': 849,\n",
       " 'AAAAAAAECEEEEIIIIDNOOOOOOOOEUUUUUYTHssaaaaaaaeceeeeiiiionooooooooeuuuuuythy': 850,\n",
       " 'ids': 851,\n",
       " 'Fpw': 852,\n",
       " 'BD': 853,\n",
       " 'Epuz': 854,\n",
       " 'Xw': 855,\n",
       " 'UOd': 856,\n",
       " 'platform': 857,\n",
       " 'win': 858,\n",
       " 're': 859,\n",
       " 'server': 860,\n",
       " 'port': 861,\n",
       " 'rm': 862,\n",
       " 'DL': 863,\n",
       " 'Http': 864,\n",
       " 'FD': 865,\n",
       " 'threading': 866,\n",
       " 'DIR': 867,\n",
       " 'SIZE': 868,\n",
       " '1024': 869,\n",
       " 'HTTPTest': 870,\n",
       " 'Request': 871,\n",
       " 'Handler': 872,\n",
       " 'Base': 873,\n",
       " 'HTTPRequest': 874,\n",
       " 'log': 875,\n",
       " 'send': 876,\n",
       " 'content': 877,\n",
       " 'total': 878,\n",
       " 'header': 879,\n",
       " 'headers': 880,\n",
       " 'mobj': 881,\n",
       " 'search': 882,\n",
       " 'group': 883,\n",
       " 'valid': 884,\n",
       " 'Content': 885,\n",
       " 'serve': 886,\n",
       " 'response': 887,\n",
       " '200': 888,\n",
       " 'mp': 889,\n",
       " 'size': 890,\n",
       " 'Length': 891,\n",
       " 'wfile': 892,\n",
       " 'write': 893,\n",
       " 'do': 894,\n",
       " 'GET': 895,\n",
       " 'regular': 896,\n",
       " 'elif': 897,\n",
       " 'Fake': 898,\n",
       " 'Logger': 899,\n",
       " 'object': 900,\n",
       " 'debug': 901,\n",
       " 'warning': 902,\n",
       " 'error': 903,\n",
       " 'httpd': 904,\n",
       " 'HTTPServer': 905,\n",
       " '127': 906,\n",
       " 'thread': 907,\n",
       " 'Thread': 908,\n",
       " 'forever': 909,\n",
       " 'daemon': 910,\n",
       " 'ep': 911,\n",
       " 'logger': 912,\n",
       " 'ydl': 913,\n",
       " 'testfile': 914,\n",
       " 'real': 915,\n",
       " 'getsize': 916,\n",
       " 'chunked': 917,\n",
       " 'chunk': 918,\n",
       " '1000': 919,\n",
       " 'abcdef': 920,\n",
       " 'Greater': 921,\n",
       " 'gettestcases': 922,\n",
       " 'report': 923,\n",
       " 'hashlib': 924,\n",
       " 'socket': 925,\n",
       " 'client': 926,\n",
       " 'urllib': 927,\n",
       " 'HTTPError': 928,\n",
       " 'Unavailable': 929,\n",
       " 'RETRIES': 930,\n",
       " 'init': 931,\n",
       " 'kwargs': 932,\n",
       " 'screen': 933,\n",
       " 'processed': 934,\n",
       " 'super': 935,\n",
       " 'md': 936,\n",
       " 'hexdigest': 937,\n",
       " 'defs': 938,\n",
       " 'multiprocess': 939,\n",
       " 'shared': 940,\n",
       " 'max': 941,\n",
       " 'Diff': 942,\n",
       " 'strclass': 943,\n",
       " 'cls': 944,\n",
       " 'add': 945,\n",
       " 'getattr': 946,\n",
       " 'Method': 947,\n",
       " 'Name': 948,\n",
       " 'generator': 949,\n",
       " 'case': 950,\n",
       " 'tname': 951,\n",
       " 'template': 952,\n",
       " 'ies': 953,\n",
       " 'playlist': 954,\n",
       " 'any': 955,\n",
       " 'k': 956,\n",
       " 'startswith': 957,\n",
       " 'cases': 958,\n",
       " 'skipping': 959,\n",
       " 'reason': 960,\n",
       " 'working': 961,\n",
       " 'marked': 962,\n",
       " 'as': 963,\n",
       " 'WORKING': 964,\n",
       " 'tc': 965,\n",
       " 'raise': 966,\n",
       " 'Exception': 967,\n",
       " 'definition': 968,\n",
       " 'incorrect': 969,\n",
       " 'output': 970,\n",
       " 'cannot': 971,\n",
       " 'be': 972,\n",
       " 'known': 973,\n",
       " 'both': 974,\n",
       " 'keys': 975,\n",
       " 'skip': 976,\n",
       " 'depends': 977,\n",
       " 'on': 978,\n",
       " 'outtmpl': 979,\n",
       " 'setdefault': 980,\n",
       " 'flat': 981,\n",
       " 'auto': 982,\n",
       " 'default': 983,\n",
       " 'finished': 984,\n",
       " 'hook': 985,\n",
       " 'called': 986,\n",
       " 'status': 987,\n",
       " 'progress': 988,\n",
       " 'prepare': 989,\n",
       " 'res': 990,\n",
       " 'tcs': 991,\n",
       " 'part': 992,\n",
       " 'splitext': 993,\n",
       " 'num': 994,\n",
       " 'while': 995,\n",
       " 'force': 996,\n",
       " 'generic': 997,\n",
       " 'err': 998,\n",
       " 'exc': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "x = './repo_files/all_tokens.pickle'\n",
    "\n",
    "with open(x, 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataClass.DataClass import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting repo tensorflow\n",
      "Starting repo .ipynb_checkpoints\n",
      "Starting repo youtube-dl\n",
      "Starting repo kubernetes\n",
      "Waiting for tokens\n"
     ]
    }
   ],
   "source": [
    "d.generate_pair_datasets('./scrape_github', tokenizing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'electron'"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path[len(path) - path[::-1].find('/'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Flask setup\n",
    "# app = Flask(__name__)\n",
    "# CORS(app)\n",
    "\n",
    "# app.config['SECRET_KEY'] = 'fe093b0354f9ba0b1237a5e36f58caf55cc9f5682c2627b7463c30f0bbd97672'  # noqa\n",
    "\n",
    "\n",
    "# Regex\n",
    "newline_ptr = re.compile(r'(?:\"[^\"]*\"|.)+')  # \\n outside of quotes (https://stackoverflow.com/questions/24018577/parsing-a-string-in-python-how-to-split-newlines-while-ignoring-newline-inside)\n",
    "comment_ptr = re.compile(r'\"\"\"(.*?)\"\"\"|\\'\\'\\'(.*?)\\'\\'\\'|#[^\"\\']*?(?=\\n)|#.*?(\".*?\"|\\'.*?\\').*?(?=\\n)', re.DOTALL|re.MULTILINE)\n",
    "literal_ptr = re.compile(r'\".*?\"|\\'.*?\\'|[-+]?\\d*\\.\\d+|\\d+')\n",
    "camelcase_ptr = re.compile(r\"(?<=[a-z])([A-Z]+[a-z]*)\")\n",
    "number_ptr = re.compile(r'(?<=[^a-zA-Z])([-+]?\\d*\\.\\d+|\\d+)')\n",
    "number_with_alpha_ptr = re.compile(r'(?<=[a-zA-Z])([-+]?\\d*\\.\\d+|\\d+)')  # split numbers from alpha\n",
    "string_ptr = re.compile(r'\".*?\"|\\'.*?\\'')\n",
    "code_ptr = re.compile(r\"([^a-zA-Z0-9])\")\n",
    "whitespace_ptr = re.compile(r\"(\\s+)\")\n",
    "\n",
    "\n",
    "# Read default options\n",
    "# parser = configargparse.ArgumentParser(description=\"index.py\")\n",
    "# opts.system_opts(parser)\n",
    "# opt = parser.parse_args()\n",
    "\n",
    "results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"scrape_github/electron\"\n",
    "\n",
    "\n",
    "# sources, num_files = read_data(opt.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scrape_github/electron'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_newlines(s):\n",
    "    \"\"\"\n",
    "    Split based on new lines (\\n) outside of quotes.\n",
    "\n",
    "    Note that this coalesces several newlines into one,\n",
    "    as blank lines are ignored. To avoid that, give a null case:\n",
    "\n",
    "    (?:\"[^\"]*\"|.)+|(?!\\Z)\n",
    "    \"\"\"\n",
    "    return re.findall(newline_ptr, s)\n",
    "\n",
    "def remove_comments(source):\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", re.sub(comment_ptr, \"\", source))\n",
    "\n",
    "\n",
    "def get_lines_from_source(source):\n",
    "    \"\"\"\n",
    "    Remove comments and empty lines from source.\n",
    "    Return a list of lines\n",
    "    \"\"\"\n",
    "    source = remove_comments(source)\n",
    "\n",
    "    lines = split_newlines(source)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_examples_line_by_line(source):  # If line, no need to filter based on length\n",
    "    examples = []\n",
    "\n",
    "    lines = get_lines_from_source(source)\n",
    "    if not lines:\n",
    "        return []\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_source(source):\n",
    "    \"\"\"\n",
    "    Remove # in strings for comment_ptr.\n",
    "\n",
    "    TODO. Later on, fix regex instead.\n",
    "    \"\"\"\n",
    "    for string in re.findall(r'\".*?#.*?\"|\\'.*?#.*?\\'', source):\n",
    "        if '#' in string:\n",
    "            modified = string.replace('#', '')\n",
    "            source = source.replace(string, modified)\n",
    "    return source\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"[!] Data does not exist\")\n",
    "    elif os.path.isfile(path):\n",
    "        return read_file(path)\n",
    "    else:\n",
    "        return read_dir(path)\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    num_file = 1\n",
    "    sources = []\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            source.append(preprocess_source(f.read()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return sources, num_file\n",
    "\n",
    "\n",
    "def read_dir(path):\n",
    "    num_file = 0\n",
    "    sources = []\n",
    "    for filename in glob.iglob(os.path.join(path, \"**/*.py\"), recursive=True):\n",
    "        if not filename.endswith('.py'): continue\n",
    "        num_file += 1\n",
    "        with open(filename, \"r\") as f:\n",
    "            sources.append(preprocess_source(f.read()))\n",
    "    return sources, num_file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = 'scrape_github/electron'\n",
    "sources, num_file = read_data(path)\n",
    "all_lines = None\n",
    "for source in sources:\n",
    "    lines = extract_examples_line_by_line(source)\n",
    "    if lines == []: continue\n",
    "    y = pd.DataFrame(lines)\n",
    "    y.columns = ['line']\n",
    "    y = y[y['line'].apply(lambda x: len(str(x).strip()) > 0)].reset_index(drop=True)    \n",
    "    x = pd.concat([pd.DataFrame([\"\"]), y['line'][:-1]]).reset_index(drop=True)\n",
    "    pair = pd.concat([x, y], axis=1)\n",
    "    all_lines = pd.concat([all_lines, pair], axis=0)# if all_lines is not None else pair\n",
    "\n",
    "all_lines = pd.concat([pd.DataFrame(np.array([all_lines.shape[0], None]).reshape(1, -1), columns=all_lines.columns), all_lines], axis=0)\n",
    "all_lines.to_csv(path[len(path) - path[::-1].find('/'):] + '_line_pairs.csv', header=None, index=None)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Dataset, ConcatDataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, chunksize):\n",
    "        \n",
    "        self.filename = filename\n",
    "        self.chunksize = chunksize\n",
    "        temp = next(pd.read_csv(self.filename, skiprows = 0, chunksize=1, header=None))\n",
    "        self.len = int(temp.values[0][0] / self.chunksize)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = next(pd.read_csv(self.filename,\n",
    "                            skiprows=idx * self.chunksize+1,\n",
    "                            chunksize=self.chunksize, header=None))\n",
    "    \n",
    "        return x.values[0][0], x.values[0][1]\n",
    "    \n",
    "\n",
    "datasets = [MyDataset('electron_line_pairs.csv', 1), MyDataset('lantern_line_pairs.csv', 1)]\n",
    "z = DataLoader(ConcatDataset(datasets), batch_size=1, shuffle=False)\n",
    "# for i, (x, y) in enumerate(z):\n",
    "# #     continue\n",
    "# # #     print(i)\n",
    "#     print(\"the x: %s and y: %s\" % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyDataset('electron_line_pairs.csv', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the x: tensor([nan], dtype=torch.float64) and y: ('import os',)\n"
     ]
    }
   ],
   "source": [
    "# z = DataLoader(ConcatDataset(datasets), batch_size=1, shuffle=False)\n",
    "for i, (x, y) in enumerate(z):\n",
    "#     continue\n",
    "# #     print(i)\n",
    "    print(\"the x: %s and y: %s\" % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-338-96a581c0152e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-338-96a581c0152e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    datasets = [MyDataset('electron_line_pairs.csv', 1, 2), MyDataset('lantern_line_pairs.csv', 1,\u001b[0m\n\u001b[0m                                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "datasets = [MyDataset('electron_line_pairs.csv', 1, 2), MyDataset('lantern_line_pairs.csv', 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset, dataset_edit = generate_datasets(opt)\n",
    "# sources, num_file = read_data(opt.path)\n",
    "# dataset = construct_dataset(sources)#generate_datasets(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>import sys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def main(args):</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for dirname in args:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>try:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>os.makedirs(dirname)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>except OSError as e:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>if e.errno == os.errno.EEXIST:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0\n",
       "0                                      \n",
       "1                             import os\n",
       "2                            import sys\n",
       "3                       def main(args):\n",
       "4                  for dirname in args:\n",
       "5                                  try:\n",
       "6                  os.makedirs(dirname)\n",
       "7                  except OSError as e:\n",
       "8        if e.errno == os.errno.EEXIST:"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import sys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def main(args):</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for dirname in args:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>try:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>os.makedirs(dirname)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>except OSError as e:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>if e.errno == os.errno.EEXIST:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>main(sys.argv[1:])</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   line\n",
       "0                             import os\n",
       "1                            import sys\n",
       "2                       def main(args):\n",
       "3                  for dirname in args:\n",
       "4                                  try:\n",
       "5                  os.makedirs(dirname)\n",
       "6                  except OSError as e:\n",
       "7        if e.errno == os.errno.EEXIST:\n",
       "9                    main(sys.argv[1:])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import sys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def main(args):</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for dirname in args:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>try:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>os.makedirs(dirname)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>except OSError as e:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>if e.errno == os.errno.EEXIST:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>main(sys.argv[1:])</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   line\n",
       "0                             import os\n",
       "1                            import sys\n",
       "2                       def main(args):\n",
       "3                  for dirname in args:\n",
       "4                                  try:\n",
       "5                  os.makedirs(dirname)\n",
       "6                  except OSError as e:\n",
       "7        if e.errno == os.errno.EEXIST:\n",
       "9                    main(sys.argv[1:])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[y['line'].apply(lambda x: len(str(x).replace(' ', '')) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        '"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['line'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: True",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-ef9675bdba12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# y = y[y.apply(lambda x: len(x) > 0)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'liney'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# df['names'].apply(lambda x: len(x)>1) &\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: True"
     ]
    }
   ],
   "source": [
    "# y = y[y.apply(lambda x: len(x) > 0)]\n",
    "y[(len(str(y['liney'])) > 0)]\n",
    "\n",
    "# df[\n",
    "# df['names'].apply(lambda x: len(x)>1) &\n",
    "# df['cars'].apply(lambda x: \"i\" in x) &\n",
    "# df['age'].apply(lambda x: int(x)<2)\n",
    "#   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_examples_line_by_line(source,\n",
    "                                  n=opt.n_leftmost_tokens,\n",
    "                                  min_len=opt.min_len):  # If line, no need to filter based on length\n",
    "    examples = []\n",
    "\n",
    "    lines = get_lines_from_source(source,\n",
    "                                  remove_comments_from_source=True,\n",
    "                                  remove_empty_lines_from_source=True)\n",
    "    if not lines:\n",
    "        return []\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "hey = extract_examples_line_by_line(dataset)\n",
    "\n",
    "y = pd.DataFrame(hey)\n",
    "x = pd.concat([pd.DataFrame([\"\"]), y[0][:-1]]).reset_index(drop=True)\n",
    "pair = pd.concat([x, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\"debug\": True, \"save\": True, \"overwrite\": True, \"verbose\": True, \"unit\": \"line\", \"min_len\": 3, \"distance_metric_x\": \"nc\",\n",
    "          \"distance_threshold_x\": 0.3, \"distance_metric_y\": \"c\", \"distance_threshold_y\": 2, \"n_leftmost_tokens\": 1, \"max_num_candidates\": 1,\n",
    "      \"check_exact_match_suffix\": True}\n",
    "from collections import namedtuple\n",
    "MyStruct = namedtuple('MyStruct', 'path debug save overwrite verbose unit min_len distance_metric_x distance_threshold_x distance_metric_y distance_threshold_y n_leftmost_tokens max_num_candidates check_exact_match_suffix')\n",
    "\n",
    "opt = MyStruct(path = path, debug = True, save = True, overwrite = True, verbose = True, unit = \"line\", min_len = 10, distance_metric_x = \"nc\",\n",
    "          distance_threshold_x = 0.3, distance_metric_y = \"c\", distance_threshold_y = 2, n_leftmost_tokens = 1, max_num_candidates = 1, \n",
    "               check_exact_match_suffix = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-7a02113e77f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1315\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "hey[1315]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Read data\n",
    "###############################################################################\n",
    "\n",
    "def preprocess_source(source):\n",
    "    \"\"\"\n",
    "    Remove # in strings for comment_ptr.\n",
    "\n",
    "    TODO. Later on, fix regex instead.\n",
    "    \"\"\"\n",
    "    for string in re.findall(r'\".*?#.*?\"|\\'.*?#.*?\\'', source):\n",
    "        if '#' in string:\n",
    "            modified = string.replace('#', '')\n",
    "            source = source.replace(string, modified)\n",
    "    return source\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"[!] Data does not exist\")\n",
    "    elif os.path.isfile(path):\n",
    "        return read_file(path)\n",
    "    else:\n",
    "        return read_dir(path)\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    num_file = 1\n",
    "    sources = []\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            source.append(preprocess_source(f.read()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return sources, num_file\n",
    "\n",
    "\n",
    "def read_dir(path):\n",
    "    num_file = 0\n",
    "    sources = []\n",
    "    for filename in glob.iglob(os.path.join(path, \"**/*.py\"), recursive=True):\n",
    "#         print(filename)\n",
    "        num_file += 1\n",
    "        with open(filename, \"r\") as f:\n",
    "            sources.append(preprocess_source(f.read()))\n",
    "    return sources, num_file\n",
    "\n",
    "\n",
    "def read_from_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_as_pickle(path, data):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_dataset(sources):\n",
    "#     \"\"\"\n",
    "#     From a project, extract all (x, y) pairs\n",
    "#     \"\"\"\n",
    "#     dataset = []\n",
    "#     num_lines = 0\n",
    "#     for source in sources:\n",
    "\n",
    "#         num_lines += len(get_lines_from_source(source,\n",
    "#                                                remove_comments_from_source=True,\n",
    "#                                                remove_empty_lines_from_source=True))\n",
    "#         return source\n",
    "#         examples = extract_examples_line_by_line(source)\n",
    "# #         print(examples)\n",
    "#         dataset.extend(examples)\n",
    "\n",
    "#     print(f\"\\n[construct_dataset] Number of lines in the project: {num_lines}\")\n",
    "#     print(f\"[construct_dataset] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(opt):\n",
    "    \"\"\"\n",
    "    Check if processed datasets exist.\n",
    "\n",
    "    If so, read. Otherwise, generate and save to the path.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[generate_datasets] Processing {opt.path}\")\n",
    "    options = to_string_opt(opt)\n",
    "    path_dataset = os.path.join(opt.path, f\"dataset_{options}.p\")\n",
    "    path_dataset_edit = os.path.join(opt.path, f\"dataset_edit_{options}.p\")\n",
    "\n",
    "    if os.path.exists(path_dataset) and os.path.exists(path_dataset_edit) and not opt.overwrite:\n",
    "        print(\"[generate_datasets] Read from pickled files\")\n",
    "        dataset = read_from_pickle(path_dataset)\n",
    "        dataset_edit = read_from_pickle(path_dataset_edit)\n",
    "    else:\n",
    "        sources, num_file = read_data(opt.path)\n",
    "\n",
    "        dataset = construct_dataset(sources)  # D_proj = {(x, y)}\n",
    "        if opt.save:\n",
    "            save_as_pickle(path_dataset, dataset)\n",
    "            save_as_pickle(path_dataset_edit, dataset_edit)\n",
    "\n",
    "    print(f\"\\n[generate_datasets] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Utils\n",
    "###############################################################################\n",
    "\n",
    "def rand_select(list, k):\n",
    "    print(f\"Randomly selected {k} examples from {len(list)} examples:\")\n",
    "    return random.choices(list, k=k)\n",
    "\n",
    "\n",
    "def strip_empty_lines(s):\n",
    "    \"\"\"\n",
    "    Remove empty lines at first and last.\n",
    "    \"\"\"\n",
    "    lines = s.splitlines()\n",
    "    while lines and not lines[0].strip():\n",
    "        lines.pop(0)\n",
    "    while lines and not lines[-1].strip():\n",
    "        lines.pop()\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def split_newlines(s):\n",
    "    \"\"\"\n",
    "    Split based on new lines (\\n) outside of quotes.\n",
    "\n",
    "    Note that this coalesces several newlines into one,\n",
    "    as blank lines are ignored. To avoid that, give a null case:\n",
    "\n",
    "    (?:\"[^\"]*\"|.)+|(?!\\Z)\n",
    "    \"\"\"\n",
    "    return re.findall(newline_ptr, s)\n",
    "\n",
    "\n",
    "def remove_null(l):\n",
    "    return list(filter(None, l))\n",
    "\n",
    "\n",
    "def remove_comments(source):\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", re.sub(comment_ptr, \"\", source))\n",
    "\n",
    "\n",
    "def remove_redundant_indentation(code):\n",
    "    lines = split_newlines(code)\n",
    "    redundant_indentation = min([len(line) - len(line.lstrip())\n",
    "                                 for line in lines\n",
    "                                 if len(line.strip()) > 0])\n",
    "    lines = [line[redundant_indentation:] for line in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def to_string_opt(opt):\n",
    "    \"\"\"\n",
    "    Generate a string for filename that contains current options\n",
    "\n",
    "    E.g. u_line__d_metric_n__d_thre_0.5__n_1__max_can_5\n",
    "    \"\"\"\n",
    "    s = []\n",
    "    s.append(f'u_{opt.unit}')\n",
    "    s.append(f'd_metric_x_{opt.distance_metric_x}')\n",
    "    s.append(f'd_thre_x_{opt.distance_threshold_x}')\n",
    "    s.append(f'd_metric_y_{opt.distance_metric_y}')\n",
    "    s.append(f'd_thre_y_{opt.distance_threshold_y}')\n",
    "    s.append(f'n_{opt.n_leftmost_tokens}')\n",
    "    s.append(f'max_can_{opt.max_num_candidates}')\n",
    "    return '__'.join(s)\n",
    "\n",
    "\n",
    "def get_lines_from_source(source,\n",
    "                          remove_comments_from_source,\n",
    "                          remove_empty_lines_from_source):\n",
    "    \"\"\"\n",
    "    Remove comments and empty lines from source.\n",
    "    Return a list of lines\n",
    "    \"\"\"\n",
    "    if remove_comments_from_source:\n",
    "        source = remove_comments(source)\n",
    "\n",
    "    lines = split_newlines(source)\n",
    "\n",
    "    if remove_empty_lines_from_source:\n",
    "        lines = [line for line in lines if len(line.strip()) > 0]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Tokenize\n",
    "###############################################################################\n",
    "\n",
    "def tokenize(s,\n",
    "             split_camelcase,\n",
    "             split_number_from_alpha,\n",
    "             keep_literal,\n",
    "             keep_whitespace,\n",
    "             verbose=False):\n",
    "\n",
    "    numbers, strings, delimiters = [], [], []\n",
    "\n",
    "    if keep_literal:\n",
    "        numbers = remove_null(set(re.findall(number_ptr, s)))\n",
    "        strings = remove_null(set(re.findall(string_ptr, s)))\n",
    "\n",
    "        literals = [f\"(?<=[^a-zA-Z0-9]){re.escape(l)}|^{re.escape(l)}\" for l in numbers]  # Add negative look ahead to exclude cases like fc1\n",
    "        literals.extend([re.escape(l) for l in strings])\n",
    "\n",
    "        delimiters = sorted(literals, key=len, reverse=True)\n",
    "\n",
    "    # Basic tokenization based on non alphanumeric tokens\n",
    "    delimiters.append(\"[^a-zA-Z0-9]\")  # Be careful of the order\n",
    "    delimiters = remove_null(delimiters)\n",
    "\n",
    "    tmp_code_ptr = \"({})\".format(\"|\".join(delimiters))\n",
    "    tokens = remove_null(re.split(tmp_code_ptr, s))\n",
    "    if verbose:\n",
    "        print('[tokenize] Basic:', tokens)\n",
    "\n",
    "    if split_camelcase:\n",
    "        before = tokens\n",
    "        tokens = []\n",
    "        for token in before:\n",
    "            if not token:\n",
    "                continue\n",
    "            elif token in numbers or token in strings:\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.extend(re.split(camelcase_ptr, token))\n",
    "        tokens = remove_null(tokens)\n",
    "        if verbose:\n",
    "            print('[tokenize] Split camel cases:', tokens)\n",
    "\n",
    "    if split_number_from_alpha:\n",
    "        before = tokens\n",
    "        tokens = []\n",
    "        for token in before:\n",
    "            if not token:\n",
    "                continue\n",
    "            elif token in numbers or token in strings:\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.extend(re.split(number_with_alpha_ptr, token))\n",
    "        tokens = remove_null(tokens)\n",
    "        if verbose:\n",
    "            print('[tokenize] Split numbers from alpha:', tokens)\n",
    "\n",
    "    if not keep_whitespace:\n",
    "        tokens = [token for token in tokens if len(token.strip()) > 0]\n",
    "        if verbose:\n",
    "            print('[tokenize] Remove whitespace:', tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_fine_grained(s, keep_whitespace=False):\n",
    "    \"\"\"\n",
    "    Tokenize as much as possible. Used when calculating edit distance.\n",
    "\n",
    "    E.g., camelCase45 = \"hi there\" -> camel, Case, 45, \", hi, there, \"\n",
    "    \"\"\"\n",
    "    return tokenize(s,\n",
    "                    split_camelcase=True,\n",
    "                    split_number_from_alpha=True,\n",
    "                    keep_literal=False,\n",
    "                    keep_whitespace=keep_whitespace)\n",
    "\n",
    "\n",
    "def tokenize_keywords(s):\n",
    "    \"\"\"\n",
    "    Tokenize as much as human-preferable. Used when generating keywords.\n",
    "\n",
    "    Do not tokenize based on literals.\n",
    "    By default, whitespace is entirely removed.\n",
    "    \"\"\"\n",
    "    return tokenize(s,\n",
    "                    split_camelcase=True,\n",
    "                    split_number_from_alpha=True,\n",
    "                    keep_literal=True,\n",
    "                    keep_whitespace=False)\n",
    "\n",
    "\n",
    "def get_prefix(y,\n",
    "               n=1):#opt.n_leftmost_tokens):\n",
    "    return tuple(tokenize_keywords(y)[:n])\n",
    "\n",
    "\n",
    "def get_suffix(x,\n",
    "               n=1):#opt.n_leftmost_tokens):\n",
    "    return tuple(tokenize_keywords(x)[-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Print stuff in color\n",
    "###############################################################################\n",
    "\n",
    "def print_example(example):\n",
    "    x, y = example\n",
    "    print(colored(x, 'red') + colored(y, 'blue'))\n",
    "\n",
    "\n",
    "def print_examples(examples):\n",
    "    print(\"-------------------------------\")\n",
    "    for example in examples:\n",
    "        print_example(example)\n",
    "        print(\"-------------------------------\")\n",
    "\n",
    "\n",
    "def print_candidates(example, candidates):\n",
    "    print('[x] -------------------------------')\n",
    "    print(colored(example[0], 'red') + colored(example[1], 'blue'))\n",
    "    for i, (edit_distance, x, y) in enumerate(candidates, 1):\n",
    "        print(f\"[{i}] {edit_distance:.2f} --------------------------\")\n",
    "        print_example((x, y))\n",
    "\n",
    "\n",
    "def print_example_edit(example_edit):\n",
    "    x, y_abs, x_prime, y_prime = example_edit\n",
    "    print_example((x_prime, y_prime))\n",
    "    print(colored('-------------------------------', 'white'))\n",
    "    print_example((x, y_abs))\n",
    "\n",
    "\n",
    "def print_dataset_edit(dataset_edit):\n",
    "    for i, example_edit in enumerate(dataset_edit, 1):\n",
    "        print(f\"[{i}] -------------------------------\")\n",
    "        print_example_edit(example_edit)\n",
    "\n",
    "\n",
    "def plot_histogram(array, title=\"\", xlabel=\"\", ylabel=\"\"):\n",
    "    n, bins, patches = plt.hist(array, bins=30, facecolor='g', alpha=0.75)\n",
    "    plt.grid(True)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Extract (x, y) pairs\n",
    "###############################################################################\n",
    "\n",
    "def process_example(x, y, n=opt.n_leftmost_tokens):\n",
    "    \"\"\"\n",
    "    Cut and paste the prefix of y at the end of x\n",
    "    \"\"\"\n",
    "    prefix_tokens = get_prefix(y, n=n)\n",
    "    tmp_prefix_ptr = '\\s*' + '\\s*'.join([re.escape(token) for token in prefix_tokens])\n",
    "    prefix_index = re.search(tmp_prefix_ptr, y).end()\n",
    "    x += '\\n' + y[:prefix_index]  # Valid only if each unit is separated by lines\n",
    "    y = y[prefix_index:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def reverse_process_example(x, y, n=opt.n_leftmost_tokens):\n",
    "    \"\"\"\n",
    "    Cut and paste the suffix of x at the beginning of y\n",
    "    \"\"\"\n",
    "    suffix_tokens = get_suffix(x, n=n)\n",
    "    tmp_suffix_ptr = '\\s*' + '\\s*'.join([re.escape(token) for token in suffix_tokens]) + '$'\n",
    "    prefix_index = re.search(tmp_suffix_ptr, x).start()\n",
    "    y = strip_empty_lines(x[prefix_index:] + y)\n",
    "    x = x[:prefix_index]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def extract_examples_line_by_line(source,\n",
    "                                  n=opt.n_leftmost_tokens,\n",
    "                                  min_len=opt.min_len):  # If line, no need to filter based on length\n",
    "    examples = []\n",
    "    def add_example(x, y):\n",
    "        x, y = process_example(x, y, n=n)\n",
    "        examples.append((x, y))\n",
    "\n",
    "    lines = get_lines_from_source(source,\n",
    "                                  remove_comments_from_source=True,\n",
    "                                  remove_empty_lines_from_source=True)\n",
    "    if not lines:\n",
    "        return []\n",
    "\n",
    "    add_example(x=\"\", y=lines[0])  # First example doesn't have context\n",
    "    for i in range(1, len(lines) - 1):\n",
    "        add_example(x=lines[i], y=lines[i + 1])\n",
    "\n",
    "    # Filter out examples that are too short\n",
    "    examples = [(x, y) for (x, y) in examples if len(x.strip()) > min_len and len(y.strip()) > min_len]\n",
    "\n",
    "    # Filter out examples that are import stmts\n",
    "    examples = [(x, y) for (x, y) in examples if 'import' not in x and 'import' not in y]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Calculate distance\n",
    "###############################################################################\n",
    "\n",
    "def abstract(s):\n",
    "    return re.sub(literal_ptr, ' ', s)\n",
    "\n",
    "\n",
    "def has_alpha(tokens):\n",
    "    return any([token.isalpha() for token in tokens])\n",
    "\n",
    "\n",
    "def iterative_levenshtein(s, t, costs=(1, 1, 1)):\n",
    "    \"\"\"\n",
    "        iterative_levenshtein(s, t) -> ldist\n",
    "        ldist is the Levenshtein distance between the strings\n",
    "        s and t.\n",
    "        For all i and j, dist[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "\n",
    "        costs: a tuple or a list with three integers (d, i, s)\n",
    "               where d defines the costs for a deletion\n",
    "                     i defines the costs for an insertion and\n",
    "                     s defines the costs for a substitution\n",
    "    \"\"\"\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    deletes, inserts, substitutes = costs\n",
    "\n",
    "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
    "    # source prefixes can be transformed into empty strings\n",
    "    # by deletions:\n",
    "    for row in range(1, rows):\n",
    "        dist[row][0] = row * deletes\n",
    "    # target prefixes can be created from an empty source string\n",
    "    # by inserting the characters\n",
    "    for col in range(1, cols):\n",
    "        dist[0][col] = col * inserts\n",
    "\n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = substitutes\n",
    "            dist[row][col] = min(dist[row-1][col] + deletes,\n",
    "                                 dist[row][col-1] + inserts,\n",
    "                                 dist[row-1][col-1] + cost) # substitution\n",
    "    return dist\n",
    "\n",
    "\n",
    "def backtrack_levenshtein(tokens1, tokens2, dist, verbose=False):\n",
    "    \"\"\"\n",
    "    Edit tokens2 to tokens1\n",
    "    \"\"\"\n",
    "    i = len(tokens1)\n",
    "    j = len(tokens2)\n",
    "\n",
    "    replaced_pairs = []  # list of (token2, token1) pairs\n",
    "    replaced_indices = [] # list of (index2, index1) pairs\n",
    "\n",
    "#     if verbose:\n",
    "#         for row in dist:\n",
    "#             print(row)\n",
    "#         print(tokens1)\n",
    "#         print(tokens2)\n",
    "\n",
    "    while i > 0 or j > 0:\n",
    "        if j <= 0:\n",
    "            if verbose:\n",
    "                print(\"Insert\", tokens1[i - 1])\n",
    "            i -= 1\n",
    "            continue\n",
    "        if i <= 0:\n",
    "            if verbose:\n",
    "                print(\"Delete\", tokens2[j - 1])\n",
    "            j -= 1\n",
    "            continue\n",
    "\n",
    "        if tokens1[i - 1] == tokens2[j - 1]:\n",
    "            if verbose:\n",
    "                print(f\"Same {tokens1[i - 1]} (i={i - 1}, j={j - 1})\")\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif j > 0 and dist[i][j] == dist[i][j - 1] + 1:\n",
    "            if verbose:\n",
    "                print(\"Delete\", tokens2[j - 1])\n",
    "            j -= 1\n",
    "        elif i > 0 and j > 0 and dist[i][j] == dist[i - 1][j - 1] + 1:\n",
    "            if verbose:\n",
    "                print(f\"Replace {tokens2[j - 1]} with {tokens1[i - 1]}\")\n",
    "            replaced_pairs.append((tokens2[j - 1], tokens1[i - 1]))\n",
    "            replaced_indices.append((j - 1, i - 1))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and dist[i][j] == dist[i - 1][j] + 1:\n",
    "            if verbose:\n",
    "                print(\"Insert\", tokens1[i - 1])\n",
    "            i -= 1\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"Error: i={i}, j={j}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return replaced_pairs, replaced_indices\n",
    "\n",
    "\n",
    "def collapse_edit_distance(tokens1, tokens2, verbose=False):\n",
    "    dist = iterative_levenshtein(tokens1, tokens2)\n",
    "    replaced_pairs, replaced_indices = backtrack_levenshtein(tokens1, tokens2, dist, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(replaced_pairs)\n",
    "\n",
    "    edit_distance = dist[-1][-1]\n",
    "    collapse = len(replaced_pairs) - len(set(replaced_pairs))  # Do not count same replacement multiple times\n",
    "    return edit_distance - collapse\n",
    "\n",
    "\n",
    "def calculate_edit_distance(code_block1,\n",
    "                            code_block2,\n",
    "                            ignore_literals,\n",
    "                            distance_metric,\n",
    "                            verbose=False):\n",
    "    if ignore_literals:  # Todo. Just ignore difference in strings if they are substentially different\n",
    "        block1 = abstract(code_block1)\n",
    "        block2 = abstract(code_block2)\n",
    "        if verbose:\n",
    "            print(\"[.] Abstracted code blocks:\")\n",
    "            print(block1.strip())\n",
    "            print(block2.strip())\n",
    "\n",
    "    else:\n",
    "        block1 = code_block1\n",
    "        block2 = code_block2\n",
    "\n",
    "    # Tokenize\n",
    "    tokens1 = tokenize_fine_grained(block1, keep_whitespace=False)\n",
    "    tokens2 = tokenize_fine_grained(block2, keep_whitespace=False)\n",
    "\n",
    "    if not tokens1 or not tokens2:\n",
    "        return float('inf')\n",
    "\n",
    "    if not has_alpha(tokens1) or not has_alpha(tokens2):\n",
    "        return float('inf')\n",
    "\n",
    "    if verbose:\n",
    "        print(tokens1)\n",
    "        print(tokens2)\n",
    "\n",
    "    # https://github.com/doukremt/distance\n",
    "    if distance_metric == \"j\":\n",
    "        return distance.jaccard(tokens1, tokens2)\n",
    "    elif distance_metric == \"l\":\n",
    "        return distance.levenshtein(tokens1, tokens2)\n",
    "    elif distance_metric == \"h\":\n",
    "        return distance.hamming(tokens1, tokens2)\n",
    "    elif distance_metric == \"s\":\n",
    "        return distance.sorensen(tokens1, tokens2)\n",
    "    elif distance_metric == \"n\":  # Normalized Levenshtein\n",
    "        return distance.nlevenshtein(tokens1, tokens2)\n",
    "    elif distance_metric == \"c\":  # Collapsed Levenshtein edit distance\n",
    "        return collapse_edit_distance(tokens1, tokens2, verbose=verbose)\n",
    "    elif distance_metric == \"nc\":  # Normalized collapsed Levenshtein edit distance\n",
    "        collapsed = collapse_edit_distance(tokens1, tokens2, verbose=verbose)\n",
    "        return collapsed / max(len(tokens1), len(tokens2))\n",
    "\n",
    "\n",
    "def replace_diff_with_placeholders(string1, string2):\n",
    "    \"\"\"\n",
    "    Replace string2-specific tokens with placeholders. Keep original values.\n",
    "\n",
    "    E.g. string1 = 'self.fc7 = (self.relu7, 4096, 4096, \"fc7\")'\n",
    "         string2 = 'self.fc8 = (self.relu8, 4096, 1000, \"fc8\")'\n",
    "\n",
    "         return 'self.fc[[8]] = (self.relu[[8]], 4096, [[1000]], \"fc[[8]]\")'\n",
    "\n",
    "    Parameters:\n",
    "        string1 (str): y\n",
    "        string2 (str): y'; to be abstracted to be consistent with y\n",
    "    \"\"\"\n",
    "    tokens1 = tokenize_fine_grained(string1, keep_whitespace=True)  # Need to keep whitespace\n",
    "    tokens2 = tokenize_fine_grained(string2, keep_whitespace=True)  # Need to keep whitespace\n",
    "\n",
    "    dist = iterative_levenshtein(tokens1, tokens2)\n",
    "    replaced_pairs, replaced_indices = backtrack_levenshtein(tokens1, tokens2, dist)\n",
    "\n",
    "    replaced = tokens2\n",
    "    for index2, index1 in replaced_indices:\n",
    "        replaced[index2] = f'[[{replaced[index2]}]]'\n",
    "    return ''.join(replaced)\n",
    "\n",
    "\n",
    "def rank_based_on_distance(opt,\n",
    "                           example,\n",
    "                           examples,\n",
    "                           include_unsatisfying_examples,\n",
    "                           exclude_same_context,\n",
    "                           exclude_same_example,\n",
    "                           n=opt.n_leftmost_tokens,\n",
    "                           check_exact_match_suffix=opt.check_exact_match_suffix,\n",
    "                           verbose=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        check_exact_match_suffix (bool): make sure that the suffix of x\n",
    "            (n_leftmost_tokens tokens) exactly matches\n",
    "        exclude_unsatisfying_examples (bool): filter out examples whose\n",
    "            edit distance is larger than distance_threshold\n",
    "        exclude_same_context (bool): exclude examples whose context exactly matches x\n",
    "    \"\"\"\n",
    "    ranked_examples = []\n",
    "    x, y = example\n",
    "    if verbose:\n",
    "        print(\"[..] x:\", x)\n",
    "\n",
    "    candidate_examples = examples\n",
    "    \n",
    "    # Select examples whose suffix of x exactly matches with that of example\n",
    "    if check_exact_match_suffix:\n",
    "        candidate_examples = []\n",
    "        x_suffix = get_suffix(x, n)\n",
    "        if verbose:\n",
    "            print(f\"[.] Enforce to have same {n} tokens as suffix: {x_suffix}\")\n",
    "        for x_prime, y_prime in examples:\n",
    "            x_prime_suffix = get_suffix(x_prime, n)\n",
    "            if verbose:\n",
    "                print(\"[..] x':\", x_prime)\n",
    "                print(\"[..] Rightmost tokens:\", x_prime_suffix, \"\\n\")\n",
    "            if x_suffix == x_prime_suffix:\n",
    "                candidate_examples.append((x_prime, y_prime))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[.] Ranking {len(candidate_examples)} examples\")\n",
    "    for x_prime, y_prime in candidate_examples:\n",
    "        edit_distance_x = calculate_edit_distance(x,\n",
    "                                                  x_prime,\n",
    "                                                  distance_metric=opt.distance_metric_x,\n",
    "                                                  ignore_literals=False)\n",
    "        edit_distance_y = calculate_edit_distance(y,\n",
    "                                                  y_prime,\n",
    "                                                  distance_metric=opt.distance_metric_y,\n",
    "                                                  ignore_literals=False)\n",
    "\n",
    "        if include_unsatisfying_examples:  # Include all examples\n",
    "            ranked_examples.append((edit_distance, x_prime, y_prime))\n",
    "        elif edit_distance_x <= opt.distance_threshold_x and edit_distance_y <= opt.distance_threshold_y:\n",
    "            if exclude_same_context and edit_distance_x == 0:\n",
    "                continue\n",
    "            if exclude_same_example and edit_distance_x == 0 and edit_distance_y == 0:\n",
    "                continue\n",
    "            ranked_examples.append((edit_distance_x, edit_distance_y, x_prime, y_prime))\n",
    "\n",
    "    return sorted(set(ranked_examples), key=lambda x:(x[1], x[0]))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Construct datasets\n",
    "###############################################################################\n",
    "\n",
    "def construct_examples_with_suffix(dataset):\n",
    "    \"\"\"\n",
    "    To speed up, construct clusters of examples based on their suffix\n",
    "    \"\"\"\n",
    "    examples_with_suffix = collections.defaultdict(list)\n",
    "    for example in dataset:\n",
    "        x = example[0]\n",
    "        x_suffix = get_suffix(x)\n",
    "        examples_with_suffix[x_suffix].append(example)\n",
    "    return examples_with_suffix\n",
    "\n",
    "\n",
    "def construct_dataset(sources):\n",
    "    \"\"\"\n",
    "    From a project, extract all (x, y) pairs\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    num_lines = 0\n",
    "    for source in sources:\n",
    "        num_lines += len(get_lines_from_source(source,\n",
    "                                               remove_comments_from_source=True,\n",
    "                                               remove_empty_lines_from_source=True))\n",
    "        examples = extract_examples_line_by_line(source)\n",
    "        dataset.extend(examples)\n",
    "\n",
    "    print(f\"\\n[construct_dataset] Number of lines in the project: {num_lines}\")\n",
    "    print(f\"[construct_dataset] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def construct_dataset_edit(opt,\n",
    "                           dataset,\n",
    "                           verbose=False):\n",
    "    \"\"\"\n",
    "    Given (x, y) pairs, generate (x, y_abs, x', y')\n",
    "    \"\"\"\n",
    "    dataset_edit = []\n",
    "    num_examples_with_candidates = 0\n",
    "    examples_with_suffix = construct_examples_with_suffix(dataset)\n",
    "\n",
    "    print(\"\\n[construct_dataset_edit] Start generating dataset for edit\")\n",
    "    for example in tqdm(dataset):\n",
    "        x, y = example\n",
    "        x_suffix = get_suffix(x)\n",
    "        candidates = rank_based_on_distance(opt,\n",
    "                                            example,\n",
    "                                            examples_with_suffix[x_suffix],\n",
    "                                            check_exact_match_suffix=False,  # No need to check if passing examples_with_suffix\n",
    "                                            include_unsatisfying_examples=False,  # Difference: filter out\n",
    "                                            exclude_same_context=False,\n",
    "                                            exclude_same_example=True,\n",
    "                                            verbose=verbose)\n",
    "        if not candidates:\n",
    "            pass\n",
    "        else:\n",
    "            num_examples_with_candidates += 1\n",
    "            for candidate in candidates[:opt.max_num_candidates]:\n",
    "                edit_distance_x, edit_distance_y, x_prime, y_prime = candidate\n",
    "                y_abs = y  # TODO\n",
    "                dataset_edit.append((x, y_abs, x_prime, y_prime))\n",
    "    print(f\"[construct_dataset_edit] Number of examples covered for edit: {num_examples_with_candidates}/{len(dataset)} ({num_examples_with_candidates/len(dataset)*100:.2f}%)\")\n",
    "    print(f\"[construct_dataset_edit] Number of examples for edit: {len(dataset_edit)} ({len(set(dataset_edit))} unique examples for edit)\")\n",
    "    return dataset_edit\n",
    "\n",
    "\n",
    "def generate_datasets(opt):\n",
    "    \"\"\"\n",
    "    Check if processed datasets exist.\n",
    "\n",
    "    If so, read. Otherwise, generate and save to the path.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[generate_datasets] Processing {opt.path}\")\n",
    "    options = to_string_opt(opt)\n",
    "    path_dataset = os.path.join(opt.path, f\"dataset_{options}.p\")\n",
    "    path_dataset_edit = os.path.join(opt.path, f\"dataset_edit_{options}.p\")\n",
    "\n",
    "    if os.path.exists(path_dataset) and os.path.exists(path_dataset_edit) and not opt.overwrite:\n",
    "        print(\"[generate_datasets] Read from pickled files\")\n",
    "        dataset = read_from_pickle(path_dataset)\n",
    "        dataset_edit = read_from_pickle(path_dataset_edit)\n",
    "    else:\n",
    "        sources, num_file = read_data(opt.path)\n",
    "\n",
    "        dataset = construct_dataset(sources)  # D_proj = {(x, y)}\n",
    "        return dataset\n",
    "        if len(dataset) > 10000:\n",
    "            print(f\"[generate_datasets] Skipping too large project (|dataset| = {len(dataset)})\")\n",
    "            dataset_edit = []\n",
    "        else:\n",
    "            dataset_edit = construct_dataset_edit(opt, dataset)  # D_edit = {(x, y, x', y')}\n",
    "\n",
    "            # Save datasets for future use\n",
    "            if opt.save:\n",
    "                save_as_pickle(path_dataset, dataset)\n",
    "                save_as_pickle(path_dataset_edit, dataset_edit)\n",
    "\n",
    "    print(f\"\\n[generate_datasets] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "    print(f\"[generate_datasets] Number of examples for edit: {len(dataset_edit)} ({len(set(dataset_edit))} unique examples)\\n\")\n",
    "\n",
    "    return dataset, dataset_edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Render html\n",
    "###############################################################################\n",
    "\n",
    "ignorable = '(\\s*\"\"\".*?\"\"\"\\s*|\\s*\\'\\'\\'.*?\\'\\'\\'\\s*|\\s)*'\n",
    "\n",
    "\n",
    "def generate_html(sources):\n",
    "    html = \"\"\n",
    "    for source in sources:\n",
    "        if len(source.strip()) == 0:\n",
    "            continue\n",
    "        html += f\"#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#\\n\\n{source}\\n\\n\"\n",
    "    return html\n",
    "\n",
    "\n",
    "def collect_code_with_edits(dataset_edit, html):\n",
    "    \"\"\"\n",
    "    Collect all exact code (~ x + y) that have example edits in dataset_edit.\n",
    "    \"\"\"\n",
    "    # TODO Ignore one liner comments starting with #\n",
    "    code_with_edits = set()\n",
    "    cnt_not_found, cnt_found = 0, 0\n",
    "    for x, y, x_prime, y_prime in dataset_edit:  # NOTE y is not abstracted\n",
    "        x, y = reverse_process_example(x, y)  # Cut and paste the suffix of x to the beginning of y\n",
    "        code_to_find_ptr = re.escape(x.strip()) + ignorable + re.escape(y.strip())  # Add ignorable\n",
    "        code_to_find_ptr = re.compile(code_to_find_ptr, re.MULTILINE|re.DOTALL)\n",
    "\n",
    "        # Find actual code from html\n",
    "        found = re.search(code_to_find_ptr, html)\n",
    "        if not found:\n",
    "            print(\"[!] code block not found in source code\")\n",
    "            print(x)\n",
    "            print(y)\n",
    "            cnt_not_found += 1\n",
    "        else:\n",
    "            code = found.group(0).strip()\n",
    "            code_with_edits.add(code)\n",
    "            cnt_found += 1\n",
    "    print(\"\\nMark code with edits in html\")\n",
    "    print(\"Found:\", cnt_found)\n",
    "    print(\"Not found:\", cnt_not_found)\n",
    "    return sorted(code_with_edits, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] Executing main function with options\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-38898812aceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Execute main function with default options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-38898812aceb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[main] Executing main function with options\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_edit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Main\n",
    "###############################################################################\n",
    "\n",
    "def main(opt):\n",
    "    print(\"[main] Executing main function with options\")\n",
    "    sources, num_files = read_data(opt.path)\n",
    "    dataset, dataset_edit = generate_datasets(opt)\n",
    "\n",
    "    html = generate_html(sources)  # For rendering\n",
    "    code_with_edits = collect_code_with_edits(dataset_edit, html)  # For rendering\n",
    "    examples_with_suffix = construct_examples_with_suffix(dataset)  # For metadata\n",
    "\n",
    "    print(\"\\nTotal number of files processed:\", num_files)\n",
    "\n",
    "    print(\"\\nDistance metric (x):\", opt.distance_metric_x)\n",
    "    print(\"Distance threshold (x):\", opt.distance_threshold_x)\n",
    "    print(\"Distance metric (y):\", opt.distance_metric_y)\n",
    "    print(\"Distance threshold (y):\", opt.distance_threshold_y)\n",
    "\n",
    "    # print(\"\\nNumber of leftmost tokens for keywords (n):\", opt.n_leftmost_tokens)\n",
    "    # print(\"Maximum number of candidates to generate example edits (k):\", opt.max_num_candidates)\n",
    "    # print(\"Enforce exact match of the suffix of x and x':\", opt.check_exact_match_suffix)\n",
    "\n",
    "    print(f\"\\nTotal number of examples: {len(dataset)}\")\n",
    "    print(f\"Total number of example edits: {len(dataset_edit)} ({len(dataset_edit) / len(dataset) * 100:.2f}%)\")\n",
    "    print(f\"Total number of code with edits: {len(code_with_edits)}\", \"\\n\")\n",
    "\n",
    "    return {\n",
    "        'sources': sources,\n",
    "        'num_files': num_files,\n",
    "        'dataset': dataset,\n",
    "        'dataset_edit': dataset_edit,\n",
    "        'html': html,\n",
    "        'code_with_edits': code_with_edits,\n",
    "        'examples_with_suffix': examples_with_suffix\n",
    "    }\n",
    "\n",
    "\n",
    "# Execute main function with default options\n",
    "results = main(opt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b9ae3311c9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/apply_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'app' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Flask\n",
    "###############################################################################\n",
    "\n",
    "@app.route(\"/apply_options\", methods=[\"POST\"])\n",
    "def apply_options():\n",
    "    options = request.get_json(force=True)\n",
    "    print(\"\\n[apply_options] Applying requested options:\", options)\n",
    "\n",
    "    # Update opt values\n",
    "    global opt\n",
    "    opt.distance_metric_x = options['distance_metric_x']\n",
    "    opt.distance_threshold_x = float(options['distance_threshold_x'])\n",
    "    opt.distance_metric_y = options['distance_metric_y']\n",
    "    opt.distance_threshold_y = float(options['distance_threshold_y'])\n",
    "\n",
    "    # Update processed data for new options\n",
    "    global results\n",
    "    results = main(opt)\n",
    "    print('HERERERERE')\n",
    "    print(results)\n",
    "    return render_template(\"index.html\",\n",
    "                           html=results['html'],\n",
    "                           dataset_size=len(results['dataset']),\n",
    "                           dataset_edit_size=len(results['dataset_edit']),\n",
    "                           coverage=f\"{len(results['dataset_edit']) / len(results['dataset']) * 100:.2f}\",\n",
    "                           distance_metric_x=opt.distance_metric_x,\n",
    "                           distance_threshold_x=opt.distance_threshold_x,\n",
    "                           distance_metric_y=opt.distance_metric_y,\n",
    "                           distance_threshold_y=opt.distance_threshold_y)\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/get_code_with_edits\", methods=[\"POST\"])\n",
    "def get_code_with_edits():\n",
    "    return jsonify(results['code_with_edits'])\n",
    "\n",
    "\n",
    "@app.route(\"/get_metadata\", methods=[\"POST\"])\n",
    "def get_metadata():\n",
    "    example = request.get_json(force=True)\n",
    "    print(example)\n",
    "    # Process example\n",
    "\n",
    "    x = example['x'].replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    y = example['y'].replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    # print(\"Pre POrcexx examples ??????\")\n",
    "    # print(x)\n",
    "    # print(y)\n",
    "    x, y = process_example(x, y)\n",
    "    # print(\"POrcexx examples ??????\")\n",
    "    # print(x)\n",
    "    # print(y)\n",
    "\n",
    "    # Get metadata\n",
    "    x_suffix = get_suffix(x)\n",
    "    print(\"about to process example edits\")\n",
    "    example_edits = rank_based_on_distance(opt,\n",
    "                                           (x, y),\n",
    "                                           results['examples_with_suffix'][x_suffix],\n",
    "                                           check_exact_match_suffix=False,  # No need to check if passing examples_with_suffix\n",
    "                                           include_unsatisfying_examples=False,\n",
    "                                           exclude_same_context=False,\n",
    "                                           exclude_same_example=True)\n",
    "    print(example_edits)\n",
    "    print(\"in between example edits\")\n",
    "    example_edits = [{\n",
    "                        'edit_distance_x': f'{edit_distance_x:.2f}',\n",
    "                        'edit_distance_y': f'{edit_distance_y:.2f}',\n",
    "                        'x': x,\n",
    "                        'y': y,\n",
    "                    }\n",
    "                    for (edit_distance_x, edit_distance_y, x, y) in example_edits]\n",
    "\n",
    "    data = {\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'example_edits': example_edits,\n",
    "    }\n",
    "    return jsonify(data)\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def intro():\n",
    "    return render_template(\"index.html\",\n",
    "                           html=results['html'],\n",
    "                           dataset_size=len(results['dataset']),\n",
    "                           dataset_edit_size=len(results['dataset_edit']),\n",
    "                           coverage=f\"{len(results['dataset_edit']) / len(results['dataset']) * 100:.2f}\",\n",
    "                           distance_metric_x=opt.distance_metric_x,\n",
    "                           distance_threshold_x=opt.distance_threshold_x,\n",
    "                           distance_metric_y=opt.distance_metric_y,\n",
    "                           distance_threshold_y=opt.distance_threshold_y,\n",
    "                           min_len=opt.min_len)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\",  # Public\n",
    "            debug=opt.debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
