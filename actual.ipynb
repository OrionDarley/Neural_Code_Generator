{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import distance\n",
    "import collections\n",
    "import configargparse\n",
    "from tqdm import tqdm\n",
    "from  more_itertools import unique_everseen\n",
    "\n",
    "# import opts\n",
    "\n",
    "from flask import Flask, render_template, url_for, redirect, jsonify, request  # noqa\n",
    "from flask_cors import CORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EditorNoRet import EditorNoRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hey = EditorNoRetrieval(num_layers=1, num_heads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 142/1691 [00:53<10:40,  2.42it/s]"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(hey.model.parameters(), lr=1e-2)\n",
    "hey.train(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(hey.data_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Flask setup\n",
    "# app = Flask(__name__)\n",
    "# CORS(app)\n",
    "\n",
    "# app.config['SECRET_KEY'] = 'fe093b0354f9ba0b1237a5e36f58caf55cc9f5682c2627b7463c30f0bbd97672'  # noqa\n",
    "\n",
    "\n",
    "# Regex\n",
    "newline_ptr = re.compile(r'(?:\"[^\"]*\"|.)+')  # \\n outside of quotes (https://stackoverflow.com/questions/24018577/parsing-a-string-in-python-how-to-split-newlines-while-ignoring-newline-inside)\n",
    "comment_ptr = re.compile(r'\"\"\"(.*?)\"\"\"|\\'\\'\\'(.*?)\\'\\'\\'|#[^\"\\']*?(?=\\n)|#.*?(\".*?\"|\\'.*?\\').*?(?=\\n)', re.DOTALL|re.MULTILINE)\n",
    "literal_ptr = re.compile(r'\".*?\"|\\'.*?\\'|[-+]?\\d*\\.\\d+|\\d+')\n",
    "camelcase_ptr = re.compile(r\"(?<=[a-z])([A-Z]+[a-z]*)\")\n",
    "number_ptr = re.compile(r'(?<=[^a-zA-Z])([-+]?\\d*\\.\\d+|\\d+)')\n",
    "number_with_alpha_ptr = re.compile(r'(?<=[a-zA-Z])([-+]?\\d*\\.\\d+|\\d+)')  # split numbers from alpha\n",
    "string_ptr = re.compile(r'\".*?\"|\\'.*?\\'')\n",
    "code_ptr = re.compile(r\"([^a-zA-Z0-9])\")\n",
    "whitespace_ptr = re.compile(r\"(\\s+)\")\n",
    "\n",
    "\n",
    "# Read default options\n",
    "# parser = configargparse.ArgumentParser(description=\"index.py\")\n",
    "# opts.system_opts(parser)\n",
    "# opt = parser.parse_args()\n",
    "\n",
    "results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataClass.torchData import createDataLoaderAllFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"scrape_github/electron\"\n",
    "\n",
    "\n",
    "dl = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_newlines(s):\n",
    "    \"\"\"\n",
    "    Split based on new lines (\\n) outside of quotes.\n",
    "\n",
    "    Note that this coalesces several newlines into one,\n",
    "    as blank lines are ignored. To avoid that, give a null case:\n",
    "\n",
    "    (?:\"[^\"]*\"|.)+|(?!\\Z)\n",
    "    \"\"\"\n",
    "    return re.findall(newline_ptr, s)\n",
    "\n",
    "def remove_comments(source):\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", re.sub(comment_ptr, \"\", source))\n",
    "\n",
    "\n",
    "def get_lines_from_source(source):\n",
    "    \"\"\"\n",
    "    Remove comments and empty lines from source.\n",
    "    Return a list of lines\n",
    "    \"\"\"\n",
    "    source = remove_comments(source)\n",
    "\n",
    "    lines = split_newlines(source)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_examples_line_by_line(source):  # If line, no need to filter based on length\n",
    "    examples = []\n",
    "\n",
    "    lines = get_lines_from_source(source)\n",
    "    if not lines:\n",
    "        return []\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(token_dict, open('token_dict.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataClass.data_utils import tokenize_fine_grained\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_source(source):\n",
    "    \"\"\"\n",
    "    Remove # in strings for comment_ptr.\n",
    "\n",
    "    TODO. Later on, fix regex instead.\n",
    "    \"\"\"\n",
    "    for string in re.findall(r'\".*?#.*?\"|\\'.*?#.*?\\'', source):\n",
    "        if '#' in string:\n",
    "            modified = string.replace('#', '')\n",
    "            source = source.replace(string, modified)\n",
    "    return source\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"[!] Data does not exist\")\n",
    "    elif os.path.isfile(path):\n",
    "        return read_file(path)\n",
    "    else:\n",
    "        return read_dir(path)\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    num_file = 1\n",
    "    sources = []\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            source.append(preprocess_source(f.read()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return sources, num_file\n",
    "\n",
    "\n",
    "def read_dir(path):\n",
    "    num_file = 0\n",
    "    sources = []\n",
    "    for filename in glob.iglob(os.path.join(path, \"**/*.py\"), recursive=True):\n",
    "        if not filename.endswith('.py'): continue\n",
    "        num_file += 1\n",
    "        with open(filename, \"r\") as f:\n",
    "            sources.append(preprocess_source(f.read()))\n",
    "    return sources, num_file\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "path = 'scrape_github/lantern'\n",
    "sources, num_file = read_data(path)\n",
    "all_lines = None\n",
    "for source in sources:\n",
    "    lines = extract_examples_line_by_line(source)\n",
    "    if lines == []: continue\n",
    "    for line in lines:\n",
    "        for token in tokenize_fine_grained(line):\n",
    "            token_dict[token] = token_dict.get(token, len(token_dict))\n",
    "    y = pd.DataFrame(lines)\n",
    "    y.columns = ['line']\n",
    "    y = y[y['line'].apply(lambda x: len(str(x).strip()) > 0)].reset_index(drop=True)    \n",
    "    x = pd.concat([pd.DataFrame([\"\"]), y['line'][:-1]]).reset_index(drop=True)\n",
    "    pair = pd.concat([x, y], axis=1)\n",
    "    all_lines = pd.concat([all_lines, pair], axis=0)# if all_lines is not None else pair\n",
    "\n",
    "all_lines = pd.concat([pd.DataFrame(np.array([all_lines.shape[0], None]).reshape(1, -1), columns=all_lines.columns), all_lines], axis=0)\n",
    "all_lines.to_csv(path[len(path) - path[::-1].find('/'):] + '_line_pairs.csv', header=None, index=None)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, Dataset, ConcatDataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, chunksize):\n",
    "        \n",
    "        self.filename = filename\n",
    "        self.chunksize = 2#chunksize\n",
    "        temp = next(pd.read_csv(self.filename, skiprows = 0, chunksize=1, header=None))\n",
    "        self.len = int(temp.values[0][0] / self.chunksize)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = next(pd.read_csv(self.filename,\n",
    "                            skiprows=idx * self.chunksize+1,\n",
    "                            chunksize=self.chunksize, header=None, dtype=str)).fillna('OSOFo').values\n",
    "        x_tokens = ['BOS'] + tokenize_fine_grained(x[0, 0]) + ['EOS']# for idx in range(self.chunksize)]\n",
    "        y_tokens = ['BOS'] + tokenize_fine_grained(x[0, 1]) + ['EOS']# for idx in range(self.chunksize)]\n",
    "\n",
    "        return x_tokens, y_tokens\n",
    "    \n",
    "    \n",
    "def batch_collate_fn(data):\n",
    "        x, y = zip(*data)#pd.DataFrame(zip(*h)).T\n",
    "        return x, y\n",
    "#         print(x)\n",
    "        # off by one in tokenize dictionary\n",
    "        x = pd.DataFrame(x).replace(token_dict).fillna(-1)+1\n",
    "        y = pd.DataFrame(y).replace(token_dict).fillna(-1)+1\n",
    "        x.insert(0, 'start', 1)\n",
    "        x.insert(x.shape[1], 'end', 2)\n",
    "        y.insert(0, 'start', 1)\n",
    "        y.insert(y.shape[1], 'end', 2)\n",
    "        batch_xs = torch.LongTensor(x.values)\n",
    "        batch_ys = torch.LongTensor(y.values)\n",
    "        return batch_xs, batch_ys\n",
    "    \n",
    "    \n",
    "batch_size = 4\n",
    "datasets = [MyDataset('electron_line_pairs.csv', batch_size), MyDataset('lantern_line_pairs.csv', batch_size)]\n",
    "z = DataLoader(ConcatDataset(datasets), batch_size=batch_size, shuffle=True, collate_fn=batch_collate_fn)\n",
    "# for i, (x, y) in enumerate(z):\n",
    "# #     continue\n",
    "# # #     print(i)\n",
    "#     print(\"the x: %s and y: %s\" % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "count = 0\n",
    "for i, (x, y) in enumerate(z):\n",
    "    h = x\n",
    "    aa = y\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['BOS',\n",
       "  'unknown',\n",
       "  '_',\n",
       "  'files',\n",
       "  '=',\n",
       "  'self',\n",
       "  '.',\n",
       "  'Get',\n",
       "  'Unknown',\n",
       "  'Files',\n",
       "  '(',\n",
       "  ')',\n",
       "  'EOS'],\n",
       " ['BOS', 'if', 'not', 'files', ':', 'EOS'],\n",
       " ['BOS', 'def', 'effective', '_', 'revpair', '(', 'repo', ')', ':', 'EOS'],\n",
       " ['BOS', 'promptadd', '(', 'ui', ',', 'repo', ',', 'f', ')', 'EOS'])"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.insert(len(x.columns), 'dprde', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\n",
       "0  24   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "1  25  25  12  26  26  26   0   0   0   0   0   0   0   0   0   0   0\n",
       "2  34  10  11  25  12  36   0   0   0   0   0   0   0   0   0   0   0\n",
       "3  13  36  15  16  25  17  36  36  25  20  36  24  22  23  24  36  36"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.DataFrame(x).replace(tokens_dict)+1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'import': 23,\n",
       " 'mimetypes': 1,\n",
       " 'self': 24,\n",
       " '.': 24,\n",
       " 'reviewer': 11,\n",
       " '=': 25,\n",
       " '[': 25,\n",
       " ']': 25,\n",
       " 'if': 33,\n",
       " 'not': 9,\n",
       " 'cl': 10,\n",
       " ':': 35,\n",
       " 'for': 12,\n",
       " 'line': 35,\n",
       " 'in': 14,\n",
       " 'w': 15,\n",
       " 'output': 16,\n",
       " '(': 35,\n",
       " ')': 35,\n",
       " 'split': 19,\n",
       " \"'\": 23,\n",
       " '\\\\': 21,\n",
       " 'n': 22,\n",
       " 'optparse': 23,\n",
       " 'cc': 24,\n",
       " 'return': 25,\n",
       " 'pending': 26,\n",
       " 'ui': 27,\n",
       " ',': 32,\n",
       " 'repo': 29,\n",
       " '*': 32,\n",
       " 'pats': 31,\n",
       " 'opts': 32,\n",
       " 'is': 33,\n",
       " 'Noise': 34}"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import', 'os']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from DataClass.data_utils import read_data, tokenize_fine_grained, get_urls_from_csv\n",
    "\n",
    "tokenize_fine_grained(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyDataset('electron_line_pairs.csv', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the x: tensor([nan], dtype=torch.float64) and y: ('import os',)\n"
     ]
    }
   ],
   "source": [
    "# z = DataLoader(ConcatDataset(datasets), batch_size=1, shuffle=False)\n",
    "for i, (x, y) in enumerate(z):\n",
    "#     continue\n",
    "# #     print(i)\n",
    "    print(\"the x: %s and y: %s\" % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-338-96a581c0152e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-338-96a581c0152e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    datasets = [MyDataset('electron_line_pairs.csv', 1, 2), MyDataset('lantern_line_pairs.csv', 1,\u001b[0m\n\u001b[0m                                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "datasets = [MyDataset('electron_line_pairs.csv', 1, 2), MyDataset('lantern_line_pairs.csv', 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset, dataset_edit = generate_datasets(opt)\n",
    "# sources, num_file = read_data(opt.path)\n",
    "# dataset = construct_dataset(sources)#generate_datasets(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>import sys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def main(args):</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for dirname in args:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>try:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>os.makedirs(dirname)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>except OSError as e:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>if e.errno == os.errno.EEXIST:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0\n",
       "0                                      \n",
       "1                             import os\n",
       "2                            import sys\n",
       "3                       def main(args):\n",
       "4                  for dirname in args:\n",
       "5                                  try:\n",
       "6                  os.makedirs(dirname)\n",
       "7                  except OSError as e:\n",
       "8        if e.errno == os.errno.EEXIST:"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import sys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def main(args):</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for dirname in args:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>try:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>os.makedirs(dirname)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>except OSError as e:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>if e.errno == os.errno.EEXIST:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>main(sys.argv[1:])</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   line\n",
       "0                             import os\n",
       "1                            import sys\n",
       "2                       def main(args):\n",
       "3                  for dirname in args:\n",
       "4                                  try:\n",
       "5                  os.makedirs(dirname)\n",
       "6                  except OSError as e:\n",
       "7        if e.errno == os.errno.EEXIST:\n",
       "9                    main(sys.argv[1:])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import sys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def main(args):</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for dirname in args:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>try:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>os.makedirs(dirname)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>except OSError as e:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>if e.errno == os.errno.EEXIST:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>main(sys.argv[1:])</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   line\n",
       "0                             import os\n",
       "1                            import sys\n",
       "2                       def main(args):\n",
       "3                  for dirname in args:\n",
       "4                                  try:\n",
       "5                  os.makedirs(dirname)\n",
       "6                  except OSError as e:\n",
       "7        if e.errno == os.errno.EEXIST:\n",
       "9                    main(sys.argv[1:])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[y['line'].apply(lambda x: len(str(x).replace(' ', '')) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        '"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['line'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: True",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-ef9675bdba12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# y = y[y.apply(lambda x: len(x) > 0)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'liney'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# df['names'].apply(lambda x: len(x)>1) &\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: True"
     ]
    }
   ],
   "source": [
    "# y = y[y.apply(lambda x: len(x) > 0)]\n",
    "y[(len(str(y['liney'])) > 0)]\n",
    "\n",
    "# df[\n",
    "# df['names'].apply(lambda x: len(x)>1) &\n",
    "# df['cars'].apply(lambda x: \"i\" in x) &\n",
    "# df['age'].apply(lambda x: int(x)<2)\n",
    "#   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_examples_line_by_line(source,\n",
    "                                  n=opt.n_leftmost_tokens,\n",
    "                                  min_len=opt.min_len):  # If line, no need to filter based on length\n",
    "    examples = []\n",
    "\n",
    "    lines = get_lines_from_source(source,\n",
    "                                  remove_comments_from_source=True,\n",
    "                                  remove_empty_lines_from_source=True)\n",
    "    if not lines:\n",
    "        return []\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "hey = extract_examples_line_by_line(dataset)\n",
    "\n",
    "y = pd.DataFrame(hey)\n",
    "x = pd.concat([pd.DataFrame([\"\"]), y[0][:-1]]).reset_index(drop=True)\n",
    "pair = pd.concat([x, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\"debug\": True, \"save\": True, \"overwrite\": True, \"verbose\": True, \"unit\": \"line\", \"min_len\": 3, \"distance_metric_x\": \"nc\",\n",
    "          \"distance_threshold_x\": 0.3, \"distance_metric_y\": \"c\", \"distance_threshold_y\": 2, \"n_leftmost_tokens\": 1, \"max_num_candidates\": 1,\n",
    "      \"check_exact_match_suffix\": True}\n",
    "from collections import namedtuple\n",
    "MyStruct = namedtuple('MyStruct', 'path debug save overwrite verbose unit min_len distance_metric_x distance_threshold_x distance_metric_y distance_threshold_y n_leftmost_tokens max_num_candidates check_exact_match_suffix')\n",
    "\n",
    "opt = MyStruct(path = path, debug = True, save = True, overwrite = True, verbose = True, unit = \"line\", min_len = 10, distance_metric_x = \"nc\",\n",
    "          distance_threshold_x = 0.3, distance_metric_y = \"c\", distance_threshold_y = 2, n_leftmost_tokens = 1, max_num_candidates = 1, \n",
    "               check_exact_match_suffix = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-7a02113e77f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1315\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "hey[1315]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Read data\n",
    "###############################################################################\n",
    "\n",
    "def preprocess_source(source):\n",
    "    \"\"\"\n",
    "    Remove # in strings for comment_ptr.\n",
    "\n",
    "    TODO. Later on, fix regex instead.\n",
    "    \"\"\"\n",
    "    for string in re.findall(r'\".*?#.*?\"|\\'.*?#.*?\\'', source):\n",
    "        if '#' in string:\n",
    "            modified = string.replace('#', '')\n",
    "            source = source.replace(string, modified)\n",
    "    return source\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"[!] Data does not exist\")\n",
    "    elif os.path.isfile(path):\n",
    "        return read_file(path)\n",
    "    else:\n",
    "        return read_dir(path)\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    num_file = 1\n",
    "    sources = []\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            source.append(preprocess_source(f.read()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return sources, num_file\n",
    "\n",
    "\n",
    "def read_dir(path):\n",
    "    num_file = 0\n",
    "    sources = []\n",
    "    for filename in glob.iglob(os.path.join(path, \"**/*.py\"), recursive=True):\n",
    "#         print(filename)\n",
    "        num_file += 1\n",
    "        with open(filename, \"r\") as f:\n",
    "            sources.append(preprocess_source(f.read()))\n",
    "    return sources, num_file\n",
    "\n",
    "\n",
    "def read_from_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_as_pickle(path, data):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_dataset(sources):\n",
    "#     \"\"\"\n",
    "#     From a project, extract all (x, y) pairs\n",
    "#     \"\"\"\n",
    "#     dataset = []\n",
    "#     num_lines = 0\n",
    "#     for source in sources:\n",
    "\n",
    "#         num_lines += len(get_lines_from_source(source,\n",
    "#                                                remove_comments_from_source=True,\n",
    "#                                                remove_empty_lines_from_source=True))\n",
    "#         return source\n",
    "#         examples = extract_examples_line_by_line(source)\n",
    "# #         print(examples)\n",
    "#         dataset.extend(examples)\n",
    "\n",
    "#     print(f\"\\n[construct_dataset] Number of lines in the project: {num_lines}\")\n",
    "#     print(f\"[construct_dataset] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(opt):\n",
    "    \"\"\"\n",
    "    Check if processed datasets exist.\n",
    "\n",
    "    If so, read. Otherwise, generate and save to the path.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[generate_datasets] Processing {opt.path}\")\n",
    "    options = to_string_opt(opt)\n",
    "    path_dataset = os.path.join(opt.path, f\"dataset_{options}.p\")\n",
    "    path_dataset_edit = os.path.join(opt.path, f\"dataset_edit_{options}.p\")\n",
    "\n",
    "    if os.path.exists(path_dataset) and os.path.exists(path_dataset_edit) and not opt.overwrite:\n",
    "        print(\"[generate_datasets] Read from pickled files\")\n",
    "        dataset = read_from_pickle(path_dataset)\n",
    "        dataset_edit = read_from_pickle(path_dataset_edit)\n",
    "    else:\n",
    "        sources, num_file = read_data(opt.path)\n",
    "\n",
    "        dataset = construct_dataset(sources)  # D_proj = {(x, y)}\n",
    "        if opt.save:\n",
    "            save_as_pickle(path_dataset, dataset)\n",
    "            save_as_pickle(path_dataset_edit, dataset_edit)\n",
    "\n",
    "    print(f\"\\n[generate_datasets] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Utils\n",
    "###############################################################################\n",
    "\n",
    "def rand_select(list, k):\n",
    "    print(f\"Randomly selected {k} examples from {len(list)} examples:\")\n",
    "    return random.choices(list, k=k)\n",
    "\n",
    "\n",
    "def strip_empty_lines(s):\n",
    "    \"\"\"\n",
    "    Remove empty lines at first and last.\n",
    "    \"\"\"\n",
    "    lines = s.splitlines()\n",
    "    while lines and not lines[0].strip():\n",
    "        lines.pop(0)\n",
    "    while lines and not lines[-1].strip():\n",
    "        lines.pop()\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def split_newlines(s):\n",
    "    \"\"\"\n",
    "    Split based on new lines (\\n) outside of quotes.\n",
    "\n",
    "    Note that this coalesces several newlines into one,\n",
    "    as blank lines are ignored. To avoid that, give a null case:\n",
    "\n",
    "    (?:\"[^\"]*\"|.)+|(?!\\Z)\n",
    "    \"\"\"\n",
    "    return re.findall(newline_ptr, s)\n",
    "\n",
    "\n",
    "def remove_null(l):\n",
    "    return list(filter(None, l))\n",
    "\n",
    "\n",
    "def remove_comments(source):\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", re.sub(comment_ptr, \"\", source))\n",
    "\n",
    "\n",
    "def remove_redundant_indentation(code):\n",
    "    lines = split_newlines(code)\n",
    "    redundant_indentation = min([len(line) - len(line.lstrip())\n",
    "                                 for line in lines\n",
    "                                 if len(line.strip()) > 0])\n",
    "    lines = [line[redundant_indentation:] for line in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def to_string_opt(opt):\n",
    "    \"\"\"\n",
    "    Generate a string for filename that contains current options\n",
    "\n",
    "    E.g. u_line__d_metric_n__d_thre_0.5__n_1__max_can_5\n",
    "    \"\"\"\n",
    "    s = []\n",
    "    s.append(f'u_{opt.unit}')\n",
    "    s.append(f'd_metric_x_{opt.distance_metric_x}')\n",
    "    s.append(f'd_thre_x_{opt.distance_threshold_x}')\n",
    "    s.append(f'd_metric_y_{opt.distance_metric_y}')\n",
    "    s.append(f'd_thre_y_{opt.distance_threshold_y}')\n",
    "    s.append(f'n_{opt.n_leftmost_tokens}')\n",
    "    s.append(f'max_can_{opt.max_num_candidates}')\n",
    "    return '__'.join(s)\n",
    "\n",
    "\n",
    "def get_lines_from_source(source,\n",
    "                          remove_comments_from_source,\n",
    "                          remove_empty_lines_from_source):\n",
    "    \"\"\"\n",
    "    Remove comments and empty lines from source.\n",
    "    Return a list of lines\n",
    "    \"\"\"\n",
    "    if remove_comments_from_source:\n",
    "        source = remove_comments(source)\n",
    "\n",
    "    lines = split_newlines(source)\n",
    "\n",
    "    if remove_empty_lines_from_source:\n",
    "        lines = [line for line in lines if len(line.strip()) > 0]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Tokenize\n",
    "###############################################################################\n",
    "\n",
    "def tokenize(s,\n",
    "             split_camelcase,\n",
    "             split_number_from_alpha,\n",
    "             keep_literal,\n",
    "             keep_whitespace,\n",
    "             verbose=False):\n",
    "\n",
    "    numbers, strings, delimiters = [], [], []\n",
    "\n",
    "    if keep_literal:\n",
    "        numbers = remove_null(set(re.findall(number_ptr, s)))\n",
    "        strings = remove_null(set(re.findall(string_ptr, s)))\n",
    "\n",
    "        literals = [f\"(?<=[^a-zA-Z0-9]){re.escape(l)}|^{re.escape(l)}\" for l in numbers]  # Add negative look ahead to exclude cases like fc1\n",
    "        literals.extend([re.escape(l) for l in strings])\n",
    "\n",
    "        delimiters = sorted(literals, key=len, reverse=True)\n",
    "\n",
    "    # Basic tokenization based on non alphanumeric tokens\n",
    "    delimiters.append(\"[^a-zA-Z0-9]\")  # Be careful of the order\n",
    "    delimiters = remove_null(delimiters)\n",
    "\n",
    "    tmp_code_ptr = \"({})\".format(\"|\".join(delimiters))\n",
    "    tokens = remove_null(re.split(tmp_code_ptr, s))\n",
    "    if verbose:\n",
    "        print('[tokenize] Basic:', tokens)\n",
    "\n",
    "    if split_camelcase:\n",
    "        before = tokens\n",
    "        tokens = []\n",
    "        for token in before:\n",
    "            if not token:\n",
    "                continue\n",
    "            elif token in numbers or token in strings:\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.extend(re.split(camelcase_ptr, token))\n",
    "        tokens = remove_null(tokens)\n",
    "        if verbose:\n",
    "            print('[tokenize] Split camel cases:', tokens)\n",
    "\n",
    "    if split_number_from_alpha:\n",
    "        before = tokens\n",
    "        tokens = []\n",
    "        for token in before:\n",
    "            if not token:\n",
    "                continue\n",
    "            elif token in numbers or token in strings:\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.extend(re.split(number_with_alpha_ptr, token))\n",
    "        tokens = remove_null(tokens)\n",
    "        if verbose:\n",
    "            print('[tokenize] Split numbers from alpha:', tokens)\n",
    "\n",
    "    if not keep_whitespace:\n",
    "        tokens = [token for token in tokens if len(token.strip()) > 0]\n",
    "        if verbose:\n",
    "            print('[tokenize] Remove whitespace:', tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_fine_grained(s, keep_whitespace=False):\n",
    "    \"\"\"\n",
    "    Tokenize as much as possible. Used when calculating edit distance.\n",
    "\n",
    "    E.g., camelCase45 = \"hi there\" -> camel, Case, 45, \", hi, there, \"\n",
    "    \"\"\"\n",
    "    return tokenize(s,\n",
    "                    split_camelcase=True,\n",
    "                    split_number_from_alpha=True,\n",
    "                    keep_literal=False,\n",
    "                    keep_whitespace=keep_whitespace)\n",
    "\n",
    "\n",
    "def tokenize_keywords(s):\n",
    "    \"\"\"\n",
    "    Tokenize as much as human-preferable. Used when generating keywords.\n",
    "\n",
    "    Do not tokenize based on literals.\n",
    "    By default, whitespace is entirely removed.\n",
    "    \"\"\"\n",
    "    return tokenize(s,\n",
    "                    split_camelcase=True,\n",
    "                    split_number_from_alpha=True,\n",
    "                    keep_literal=True,\n",
    "                    keep_whitespace=False)\n",
    "\n",
    "\n",
    "def get_prefix(y,\n",
    "               n=1):#opt.n_leftmost_tokens):\n",
    "    return tuple(tokenize_keywords(y)[:n])\n",
    "\n",
    "\n",
    "def get_suffix(x,\n",
    "               n=1):#opt.n_leftmost_tokens):\n",
    "    return tuple(tokenize_keywords(x)[-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Print stuff in color\n",
    "###############################################################################\n",
    "\n",
    "def print_example(example):\n",
    "    x, y = example\n",
    "    print(colored(x, 'red') + colored(y, 'blue'))\n",
    "\n",
    "\n",
    "def print_examples(examples):\n",
    "    print(\"-------------------------------\")\n",
    "    for example in examples:\n",
    "        print_example(example)\n",
    "        print(\"-------------------------------\")\n",
    "\n",
    "\n",
    "def print_candidates(example, candidates):\n",
    "    print('[x] -------------------------------')\n",
    "    print(colored(example[0], 'red') + colored(example[1], 'blue'))\n",
    "    for i, (edit_distance, x, y) in enumerate(candidates, 1):\n",
    "        print(f\"[{i}] {edit_distance:.2f} --------------------------\")\n",
    "        print_example((x, y))\n",
    "\n",
    "\n",
    "def print_example_edit(example_edit):\n",
    "    x, y_abs, x_prime, y_prime = example_edit\n",
    "    print_example((x_prime, y_prime))\n",
    "    print(colored('-------------------------------', 'white'))\n",
    "    print_example((x, y_abs))\n",
    "\n",
    "\n",
    "def print_dataset_edit(dataset_edit):\n",
    "    for i, example_edit in enumerate(dataset_edit, 1):\n",
    "        print(f\"[{i}] -------------------------------\")\n",
    "        print_example_edit(example_edit)\n",
    "\n",
    "\n",
    "def plot_histogram(array, title=\"\", xlabel=\"\", ylabel=\"\"):\n",
    "    n, bins, patches = plt.hist(array, bins=30, facecolor='g', alpha=0.75)\n",
    "    plt.grid(True)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Extract (x, y) pairs\n",
    "###############################################################################\n",
    "\n",
    "def process_example(x, y, n=opt.n_leftmost_tokens):\n",
    "    \"\"\"\n",
    "    Cut and paste the prefix of y at the end of x\n",
    "    \"\"\"\n",
    "    prefix_tokens = get_prefix(y, n=n)\n",
    "    tmp_prefix_ptr = '\\s*' + '\\s*'.join([re.escape(token) for token in prefix_tokens])\n",
    "    prefix_index = re.search(tmp_prefix_ptr, y).end()\n",
    "    x += '\\n' + y[:prefix_index]  # Valid only if each unit is separated by lines\n",
    "    y = y[prefix_index:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def reverse_process_example(x, y, n=opt.n_leftmost_tokens):\n",
    "    \"\"\"\n",
    "    Cut and paste the suffix of x at the beginning of y\n",
    "    \"\"\"\n",
    "    suffix_tokens = get_suffix(x, n=n)\n",
    "    tmp_suffix_ptr = '\\s*' + '\\s*'.join([re.escape(token) for token in suffix_tokens]) + '$'\n",
    "    prefix_index = re.search(tmp_suffix_ptr, x).start()\n",
    "    y = strip_empty_lines(x[prefix_index:] + y)\n",
    "    x = x[:prefix_index]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def extract_examples_line_by_line(source,\n",
    "                                  n=opt.n_leftmost_tokens,\n",
    "                                  min_len=opt.min_len):  # If line, no need to filter based on length\n",
    "    examples = []\n",
    "    def add_example(x, y):\n",
    "        x, y = process_example(x, y, n=n)\n",
    "        examples.append((x, y))\n",
    "\n",
    "    lines = get_lines_from_source(source,\n",
    "                                  remove_comments_from_source=True,\n",
    "                                  remove_empty_lines_from_source=True)\n",
    "    if not lines:\n",
    "        return []\n",
    "\n",
    "    add_example(x=\"\", y=lines[0])  # First example doesn't have context\n",
    "    for i in range(1, len(lines) - 1):\n",
    "        add_example(x=lines[i], y=lines[i + 1])\n",
    "\n",
    "    # Filter out examples that are too short\n",
    "    examples = [(x, y) for (x, y) in examples if len(x.strip()) > min_len and len(y.strip()) > min_len]\n",
    "\n",
    "    # Filter out examples that are import stmts\n",
    "    examples = [(x, y) for (x, y) in examples if 'import' not in x and 'import' not in y]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Calculate distance\n",
    "###############################################################################\n",
    "\n",
    "def abstract(s):\n",
    "    return re.sub(literal_ptr, ' ', s)\n",
    "\n",
    "\n",
    "def has_alpha(tokens):\n",
    "    return any([token.isalpha() for token in tokens])\n",
    "\n",
    "\n",
    "def iterative_levenshtein(s, t, costs=(1, 1, 1)):\n",
    "    \"\"\"\n",
    "        iterative_levenshtein(s, t) -> ldist\n",
    "        ldist is the Levenshtein distance between the strings\n",
    "        s and t.\n",
    "        For all i and j, dist[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "\n",
    "        costs: a tuple or a list with three integers (d, i, s)\n",
    "               where d defines the costs for a deletion\n",
    "                     i defines the costs for an insertion and\n",
    "                     s defines the costs for a substitution\n",
    "    \"\"\"\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    deletes, inserts, substitutes = costs\n",
    "\n",
    "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
    "    # source prefixes can be transformed into empty strings\n",
    "    # by deletions:\n",
    "    for row in range(1, rows):\n",
    "        dist[row][0] = row * deletes\n",
    "    # target prefixes can be created from an empty source string\n",
    "    # by inserting the characters\n",
    "    for col in range(1, cols):\n",
    "        dist[0][col] = col * inserts\n",
    "\n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = substitutes\n",
    "            dist[row][col] = min(dist[row-1][col] + deletes,\n",
    "                                 dist[row][col-1] + inserts,\n",
    "                                 dist[row-1][col-1] + cost) # substitution\n",
    "    return dist\n",
    "\n",
    "\n",
    "def backtrack_levenshtein(tokens1, tokens2, dist, verbose=False):\n",
    "    \"\"\"\n",
    "    Edit tokens2 to tokens1\n",
    "    \"\"\"\n",
    "    i = len(tokens1)\n",
    "    j = len(tokens2)\n",
    "\n",
    "    replaced_pairs = []  # list of (token2, token1) pairs\n",
    "    replaced_indices = [] # list of (index2, index1) pairs\n",
    "\n",
    "#     if verbose:\n",
    "#         for row in dist:\n",
    "#             print(row)\n",
    "#         print(tokens1)\n",
    "#         print(tokens2)\n",
    "\n",
    "    while i > 0 or j > 0:\n",
    "        if j <= 0:\n",
    "            if verbose:\n",
    "                print(\"Insert\", tokens1[i - 1])\n",
    "            i -= 1\n",
    "            continue\n",
    "        if i <= 0:\n",
    "            if verbose:\n",
    "                print(\"Delete\", tokens2[j - 1])\n",
    "            j -= 1\n",
    "            continue\n",
    "\n",
    "        if tokens1[i - 1] == tokens2[j - 1]:\n",
    "            if verbose:\n",
    "                print(f\"Same {tokens1[i - 1]} (i={i - 1}, j={j - 1})\")\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif j > 0 and dist[i][j] == dist[i][j - 1] + 1:\n",
    "            if verbose:\n",
    "                print(\"Delete\", tokens2[j - 1])\n",
    "            j -= 1\n",
    "        elif i > 0 and j > 0 and dist[i][j] == dist[i - 1][j - 1] + 1:\n",
    "            if verbose:\n",
    "                print(f\"Replace {tokens2[j - 1]} with {tokens1[i - 1]}\")\n",
    "            replaced_pairs.append((tokens2[j - 1], tokens1[i - 1]))\n",
    "            replaced_indices.append((j - 1, i - 1))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and dist[i][j] == dist[i - 1][j] + 1:\n",
    "            if verbose:\n",
    "                print(\"Insert\", tokens1[i - 1])\n",
    "            i -= 1\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"Error: i={i}, j={j}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return replaced_pairs, replaced_indices\n",
    "\n",
    "\n",
    "def collapse_edit_distance(tokens1, tokens2, verbose=False):\n",
    "    dist = iterative_levenshtein(tokens1, tokens2)\n",
    "    replaced_pairs, replaced_indices = backtrack_levenshtein(tokens1, tokens2, dist, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(replaced_pairs)\n",
    "\n",
    "    edit_distance = dist[-1][-1]\n",
    "    collapse = len(replaced_pairs) - len(set(replaced_pairs))  # Do not count same replacement multiple times\n",
    "    return edit_distance - collapse\n",
    "\n",
    "\n",
    "def calculate_edit_distance(code_block1,\n",
    "                            code_block2,\n",
    "                            ignore_literals,\n",
    "                            distance_metric,\n",
    "                            verbose=False):\n",
    "    if ignore_literals:  # Todo. Just ignore difference in strings if they are substentially different\n",
    "        block1 = abstract(code_block1)\n",
    "        block2 = abstract(code_block2)\n",
    "        if verbose:\n",
    "            print(\"[.] Abstracted code blocks:\")\n",
    "            print(block1.strip())\n",
    "            print(block2.strip())\n",
    "\n",
    "    else:\n",
    "        block1 = code_block1\n",
    "        block2 = code_block2\n",
    "\n",
    "    # Tokenize\n",
    "    tokens1 = tokenize_fine_grained(block1, keep_whitespace=False)\n",
    "    tokens2 = tokenize_fine_grained(block2, keep_whitespace=False)\n",
    "\n",
    "    if not tokens1 or not tokens2:\n",
    "        return float('inf')\n",
    "\n",
    "    if not has_alpha(tokens1) or not has_alpha(tokens2):\n",
    "        return float('inf')\n",
    "\n",
    "    if verbose:\n",
    "        print(tokens1)\n",
    "        print(tokens2)\n",
    "\n",
    "    # https://github.com/doukremt/distance\n",
    "    if distance_metric == \"j\":\n",
    "        return distance.jaccard(tokens1, tokens2)\n",
    "    elif distance_metric == \"l\":\n",
    "        return distance.levenshtein(tokens1, tokens2)\n",
    "    elif distance_metric == \"h\":\n",
    "        return distance.hamming(tokens1, tokens2)\n",
    "    elif distance_metric == \"s\":\n",
    "        return distance.sorensen(tokens1, tokens2)\n",
    "    elif distance_metric == \"n\":  # Normalized Levenshtein\n",
    "        return distance.nlevenshtein(tokens1, tokens2)\n",
    "    elif distance_metric == \"c\":  # Collapsed Levenshtein edit distance\n",
    "        return collapse_edit_distance(tokens1, tokens2, verbose=verbose)\n",
    "    elif distance_metric == \"nc\":  # Normalized collapsed Levenshtein edit distance\n",
    "        collapsed = collapse_edit_distance(tokens1, tokens2, verbose=verbose)\n",
    "        return collapsed / max(len(tokens1), len(tokens2))\n",
    "\n",
    "\n",
    "def replace_diff_with_placeholders(string1, string2):\n",
    "    \"\"\"\n",
    "    Replace string2-specific tokens with placeholders. Keep original values.\n",
    "\n",
    "    E.g. string1 = 'self.fc7 = (self.relu7, 4096, 4096, \"fc7\")'\n",
    "         string2 = 'self.fc8 = (self.relu8, 4096, 1000, \"fc8\")'\n",
    "\n",
    "         return 'self.fc[[8]] = (self.relu[[8]], 4096, [[1000]], \"fc[[8]]\")'\n",
    "\n",
    "    Parameters:\n",
    "        string1 (str): y\n",
    "        string2 (str): y'; to be abstracted to be consistent with y\n",
    "    \"\"\"\n",
    "    tokens1 = tokenize_fine_grained(string1, keep_whitespace=True)  # Need to keep whitespace\n",
    "    tokens2 = tokenize_fine_grained(string2, keep_whitespace=True)  # Need to keep whitespace\n",
    "\n",
    "    dist = iterative_levenshtein(tokens1, tokens2)\n",
    "    replaced_pairs, replaced_indices = backtrack_levenshtein(tokens1, tokens2, dist)\n",
    "\n",
    "    replaced = tokens2\n",
    "    for index2, index1 in replaced_indices:\n",
    "        replaced[index2] = f'[[{replaced[index2]}]]'\n",
    "    return ''.join(replaced)\n",
    "\n",
    "\n",
    "def rank_based_on_distance(opt,\n",
    "                           example,\n",
    "                           examples,\n",
    "                           include_unsatisfying_examples,\n",
    "                           exclude_same_context,\n",
    "                           exclude_same_example,\n",
    "                           n=opt.n_leftmost_tokens,\n",
    "                           check_exact_match_suffix=opt.check_exact_match_suffix,\n",
    "                           verbose=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        check_exact_match_suffix (bool): make sure that the suffix of x\n",
    "            (n_leftmost_tokens tokens) exactly matches\n",
    "        exclude_unsatisfying_examples (bool): filter out examples whose\n",
    "            edit distance is larger than distance_threshold\n",
    "        exclude_same_context (bool): exclude examples whose context exactly matches x\n",
    "    \"\"\"\n",
    "    ranked_examples = []\n",
    "    x, y = example\n",
    "    if verbose:\n",
    "        print(\"[..] x:\", x)\n",
    "\n",
    "    candidate_examples = examples\n",
    "    \n",
    "    # Select examples whose suffix of x exactly matches with that of example\n",
    "    if check_exact_match_suffix:\n",
    "        candidate_examples = []\n",
    "        x_suffix = get_suffix(x, n)\n",
    "        if verbose:\n",
    "            print(f\"[.] Enforce to have same {n} tokens as suffix: {x_suffix}\")\n",
    "        for x_prime, y_prime in examples:\n",
    "            x_prime_suffix = get_suffix(x_prime, n)\n",
    "            if verbose:\n",
    "                print(\"[..] x':\", x_prime)\n",
    "                print(\"[..] Rightmost tokens:\", x_prime_suffix, \"\\n\")\n",
    "            if x_suffix == x_prime_suffix:\n",
    "                candidate_examples.append((x_prime, y_prime))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[.] Ranking {len(candidate_examples)} examples\")\n",
    "    for x_prime, y_prime in candidate_examples:\n",
    "        edit_distance_x = calculate_edit_distance(x,\n",
    "                                                  x_prime,\n",
    "                                                  distance_metric=opt.distance_metric_x,\n",
    "                                                  ignore_literals=False)\n",
    "        edit_distance_y = calculate_edit_distance(y,\n",
    "                                                  y_prime,\n",
    "                                                  distance_metric=opt.distance_metric_y,\n",
    "                                                  ignore_literals=False)\n",
    "\n",
    "        if include_unsatisfying_examples:  # Include all examples\n",
    "            ranked_examples.append((edit_distance, x_prime, y_prime))\n",
    "        elif edit_distance_x <= opt.distance_threshold_x and edit_distance_y <= opt.distance_threshold_y:\n",
    "            if exclude_same_context and edit_distance_x == 0:\n",
    "                continue\n",
    "            if exclude_same_example and edit_distance_x == 0 and edit_distance_y == 0:\n",
    "                continue\n",
    "            ranked_examples.append((edit_distance_x, edit_distance_y, x_prime, y_prime))\n",
    "\n",
    "    return sorted(set(ranked_examples), key=lambda x:(x[1], x[0]))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Construct datasets\n",
    "###############################################################################\n",
    "\n",
    "def construct_examples_with_suffix(dataset):\n",
    "    \"\"\"\n",
    "    To speed up, construct clusters of examples based on their suffix\n",
    "    \"\"\"\n",
    "    examples_with_suffix = collections.defaultdict(list)\n",
    "    for example in dataset:\n",
    "        x = example[0]\n",
    "        x_suffix = get_suffix(x)\n",
    "        examples_with_suffix[x_suffix].append(example)\n",
    "    return examples_with_suffix\n",
    "\n",
    "\n",
    "def construct_dataset(sources):\n",
    "    \"\"\"\n",
    "    From a project, extract all (x, y) pairs\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    num_lines = 0\n",
    "    for source in sources:\n",
    "        num_lines += len(get_lines_from_source(source,\n",
    "                                               remove_comments_from_source=True,\n",
    "                                               remove_empty_lines_from_source=True))\n",
    "        examples = extract_examples_line_by_line(source)\n",
    "        dataset.extend(examples)\n",
    "\n",
    "    print(f\"\\n[construct_dataset] Number of lines in the project: {num_lines}\")\n",
    "    print(f\"[construct_dataset] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def construct_dataset_edit(opt,\n",
    "                           dataset,\n",
    "                           verbose=False):\n",
    "    \"\"\"\n",
    "    Given (x, y) pairs, generate (x, y_abs, x', y')\n",
    "    \"\"\"\n",
    "    dataset_edit = []\n",
    "    num_examples_with_candidates = 0\n",
    "    examples_with_suffix = construct_examples_with_suffix(dataset)\n",
    "\n",
    "    print(\"\\n[construct_dataset_edit] Start generating dataset for edit\")\n",
    "    for example in tqdm(dataset):\n",
    "        x, y = example\n",
    "        x_suffix = get_suffix(x)\n",
    "        candidates = rank_based_on_distance(opt,\n",
    "                                            example,\n",
    "                                            examples_with_suffix[x_suffix],\n",
    "                                            check_exact_match_suffix=False,  # No need to check if passing examples_with_suffix\n",
    "                                            include_unsatisfying_examples=False,  # Difference: filter out\n",
    "                                            exclude_same_context=False,\n",
    "                                            exclude_same_example=True,\n",
    "                                            verbose=verbose)\n",
    "        if not candidates:\n",
    "            pass\n",
    "        else:\n",
    "            num_examples_with_candidates += 1\n",
    "            for candidate in candidates[:opt.max_num_candidates]:\n",
    "                edit_distance_x, edit_distance_y, x_prime, y_prime = candidate\n",
    "                y_abs = y  # TODO\n",
    "                dataset_edit.append((x, y_abs, x_prime, y_prime))\n",
    "    print(f\"[construct_dataset_edit] Number of examples covered for edit: {num_examples_with_candidates}/{len(dataset)} ({num_examples_with_candidates/len(dataset)*100:.2f}%)\")\n",
    "    print(f\"[construct_dataset_edit] Number of examples for edit: {len(dataset_edit)} ({len(set(dataset_edit))} unique examples for edit)\")\n",
    "    return dataset_edit\n",
    "\n",
    "\n",
    "def generate_datasets(opt):\n",
    "    \"\"\"\n",
    "    Check if processed datasets exist.\n",
    "\n",
    "    If so, read. Otherwise, generate and save to the path.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[generate_datasets] Processing {opt.path}\")\n",
    "    options = to_string_opt(opt)\n",
    "    path_dataset = os.path.join(opt.path, f\"dataset_{options}.p\")\n",
    "    path_dataset_edit = os.path.join(opt.path, f\"dataset_edit_{options}.p\")\n",
    "\n",
    "    if os.path.exists(path_dataset) and os.path.exists(path_dataset_edit) and not opt.overwrite:\n",
    "        print(\"[generate_datasets] Read from pickled files\")\n",
    "        dataset = read_from_pickle(path_dataset)\n",
    "        dataset_edit = read_from_pickle(path_dataset_edit)\n",
    "    else:\n",
    "        sources, num_file = read_data(opt.path)\n",
    "\n",
    "        dataset = construct_dataset(sources)  # D_proj = {(x, y)}\n",
    "        return dataset\n",
    "        if len(dataset) > 10000:\n",
    "            print(f\"[generate_datasets] Skipping too large project (|dataset| = {len(dataset)})\")\n",
    "            dataset_edit = []\n",
    "        else:\n",
    "            dataset_edit = construct_dataset_edit(opt, dataset)  # D_edit = {(x, y, x', y')}\n",
    "\n",
    "            # Save datasets for future use\n",
    "            if opt.save:\n",
    "                save_as_pickle(path_dataset, dataset)\n",
    "                save_as_pickle(path_dataset_edit, dataset_edit)\n",
    "\n",
    "    print(f\"\\n[generate_datasets] Number of examples: {len(dataset)} ({len(set(dataset))} unique examples)\")\n",
    "    print(f\"[generate_datasets] Number of examples for edit: {len(dataset_edit)} ({len(set(dataset_edit))} unique examples)\\n\")\n",
    "\n",
    "    return dataset, dataset_edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Render html\n",
    "###############################################################################\n",
    "\n",
    "ignorable = '(\\s*\"\"\".*?\"\"\"\\s*|\\s*\\'\\'\\'.*?\\'\\'\\'\\s*|\\s)*'\n",
    "\n",
    "\n",
    "def generate_html(sources):\n",
    "    html = \"\"\n",
    "    for source in sources:\n",
    "        if len(source.strip()) == 0:\n",
    "            continue\n",
    "        html += f\"#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#\\n\\n{source}\\n\\n\"\n",
    "    return html\n",
    "\n",
    "\n",
    "def collect_code_with_edits(dataset_edit, html):\n",
    "    \"\"\"\n",
    "    Collect all exact code (~ x + y) that have example edits in dataset_edit.\n",
    "    \"\"\"\n",
    "    # TODO Ignore one liner comments starting with #\n",
    "    code_with_edits = set()\n",
    "    cnt_not_found, cnt_found = 0, 0\n",
    "    for x, y, x_prime, y_prime in dataset_edit:  # NOTE y is not abstracted\n",
    "        x, y = reverse_process_example(x, y)  # Cut and paste the suffix of x to the beginning of y\n",
    "        code_to_find_ptr = re.escape(x.strip()) + ignorable + re.escape(y.strip())  # Add ignorable\n",
    "        code_to_find_ptr = re.compile(code_to_find_ptr, re.MULTILINE|re.DOTALL)\n",
    "\n",
    "        # Find actual code from html\n",
    "        found = re.search(code_to_find_ptr, html)\n",
    "        if not found:\n",
    "            print(\"[!] code block not found in source code\")\n",
    "            print(x)\n",
    "            print(y)\n",
    "            cnt_not_found += 1\n",
    "        else:\n",
    "            code = found.group(0).strip()\n",
    "            code_with_edits.add(code)\n",
    "            cnt_found += 1\n",
    "    print(\"\\nMark code with edits in html\")\n",
    "    print(\"Found:\", cnt_found)\n",
    "    print(\"Not found:\", cnt_not_found)\n",
    "    return sorted(code_with_edits, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] Executing main function with options\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-38898812aceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Execute main function with default options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-38898812aceb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[main] Executing main function with options\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_edit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Main\n",
    "###############################################################################\n",
    "\n",
    "def main(opt):\n",
    "    print(\"[main] Executing main function with options\")\n",
    "    sources, num_files = read_data(opt.path)\n",
    "    dataset, dataset_edit = generate_datasets(opt)\n",
    "\n",
    "    html = generate_html(sources)  # For rendering\n",
    "    code_with_edits = collect_code_with_edits(dataset_edit, html)  # For rendering\n",
    "    examples_with_suffix = construct_examples_with_suffix(dataset)  # For metadata\n",
    "\n",
    "    print(\"\\nTotal number of files processed:\", num_files)\n",
    "\n",
    "    print(\"\\nDistance metric (x):\", opt.distance_metric_x)\n",
    "    print(\"Distance threshold (x):\", opt.distance_threshold_x)\n",
    "    print(\"Distance metric (y):\", opt.distance_metric_y)\n",
    "    print(\"Distance threshold (y):\", opt.distance_threshold_y)\n",
    "\n",
    "    # print(\"\\nNumber of leftmost tokens for keywords (n):\", opt.n_leftmost_tokens)\n",
    "    # print(\"Maximum number of candidates to generate example edits (k):\", opt.max_num_candidates)\n",
    "    # print(\"Enforce exact match of the suffix of x and x':\", opt.check_exact_match_suffix)\n",
    "\n",
    "    print(f\"\\nTotal number of examples: {len(dataset)}\")\n",
    "    print(f\"Total number of example edits: {len(dataset_edit)} ({len(dataset_edit) / len(dataset) * 100:.2f}%)\")\n",
    "    print(f\"Total number of code with edits: {len(code_with_edits)}\", \"\\n\")\n",
    "\n",
    "    return {\n",
    "        'sources': sources,\n",
    "        'num_files': num_files,\n",
    "        'dataset': dataset,\n",
    "        'dataset_edit': dataset_edit,\n",
    "        'html': html,\n",
    "        'code_with_edits': code_with_edits,\n",
    "        'examples_with_suffix': examples_with_suffix\n",
    "    }\n",
    "\n",
    "\n",
    "# Execute main function with default options\n",
    "results = main(opt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b9ae3311c9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/apply_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'app' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Flask\n",
    "###############################################################################\n",
    "\n",
    "@app.route(\"/apply_options\", methods=[\"POST\"])\n",
    "def apply_options():\n",
    "    options = request.get_json(force=True)\n",
    "    print(\"\\n[apply_options] Applying requested options:\", options)\n",
    "\n",
    "    # Update opt values\n",
    "    global opt\n",
    "    opt.distance_metric_x = options['distance_metric_x']\n",
    "    opt.distance_threshold_x = float(options['distance_threshold_x'])\n",
    "    opt.distance_metric_y = options['distance_metric_y']\n",
    "    opt.distance_threshold_y = float(options['distance_threshold_y'])\n",
    "\n",
    "    # Update processed data for new options\n",
    "    global results\n",
    "    results = main(opt)\n",
    "    print('HERERERERE')\n",
    "    print(results)\n",
    "    return render_template(\"index.html\",\n",
    "                           html=results['html'],\n",
    "                           dataset_size=len(results['dataset']),\n",
    "                           dataset_edit_size=len(results['dataset_edit']),\n",
    "                           coverage=f\"{len(results['dataset_edit']) / len(results['dataset']) * 100:.2f}\",\n",
    "                           distance_metric_x=opt.distance_metric_x,\n",
    "                           distance_threshold_x=opt.distance_threshold_x,\n",
    "                           distance_metric_y=opt.distance_metric_y,\n",
    "                           distance_threshold_y=opt.distance_threshold_y)\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/get_code_with_edits\", methods=[\"POST\"])\n",
    "def get_code_with_edits():\n",
    "    return jsonify(results['code_with_edits'])\n",
    "\n",
    "\n",
    "@app.route(\"/get_metadata\", methods=[\"POST\"])\n",
    "def get_metadata():\n",
    "    example = request.get_json(force=True)\n",
    "    print(example)\n",
    "    # Process example\n",
    "\n",
    "    x = example['x'].replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    y = example['y'].replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    # print(\"Pre POrcexx examples ??????\")\n",
    "    # print(x)\n",
    "    # print(y)\n",
    "    x, y = process_example(x, y)\n",
    "    # print(\"POrcexx examples ??????\")\n",
    "    # print(x)\n",
    "    # print(y)\n",
    "\n",
    "    # Get metadata\n",
    "    x_suffix = get_suffix(x)\n",
    "    print(\"about to process example edits\")\n",
    "    example_edits = rank_based_on_distance(opt,\n",
    "                                           (x, y),\n",
    "                                           results['examples_with_suffix'][x_suffix],\n",
    "                                           check_exact_match_suffix=False,  # No need to check if passing examples_with_suffix\n",
    "                                           include_unsatisfying_examples=False,\n",
    "                                           exclude_same_context=False,\n",
    "                                           exclude_same_example=True)\n",
    "    print(example_edits)\n",
    "    print(\"in between example edits\")\n",
    "    example_edits = [{\n",
    "                        'edit_distance_x': f'{edit_distance_x:.2f}',\n",
    "                        'edit_distance_y': f'{edit_distance_y:.2f}',\n",
    "                        'x': x,\n",
    "                        'y': y,\n",
    "                    }\n",
    "                    for (edit_distance_x, edit_distance_y, x, y) in example_edits]\n",
    "\n",
    "    data = {\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'example_edits': example_edits,\n",
    "    }\n",
    "    return jsonify(data)\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def intro():\n",
    "    return render_template(\"index.html\",\n",
    "                           html=results['html'],\n",
    "                           dataset_size=len(results['dataset']),\n",
    "                           dataset_edit_size=len(results['dataset_edit']),\n",
    "                           coverage=f\"{len(results['dataset_edit']) / len(results['dataset']) * 100:.2f}\",\n",
    "                           distance_metric_x=opt.distance_metric_x,\n",
    "                           distance_threshold_x=opt.distance_threshold_x,\n",
    "                           distance_metric_y=opt.distance_metric_y,\n",
    "                           distance_threshold_y=opt.distance_threshold_y,\n",
    "                           min_len=opt.min_len)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\",  # Public\n",
    "            debug=opt.debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
